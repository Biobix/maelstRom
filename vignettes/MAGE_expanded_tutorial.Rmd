---
title: "MAGE expanded tutorial"
author: "Cedric Stroobandt"
date: "`r Sys.Date()`"
bibliography: REFERENCES.bib
output:
  rmarkdown::html_vignette: default
  html_document:
    code_download: yes
    theme: cosmo
    toc: yes
    toc_float: yes
    highlight: tango
    number_sections: yes
vignette: |
  %\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
options(width = 16)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# MAGE: Modeller of Allelic Gene Expression

The MAGE package provides extensive functions for RNAseq-based allelic analyses, ranging from basic tasks such as genotyping - albeit allele-specific-expression (ASE) aware and relying on RNAseq data only - to the analysis of more complex ASE-related phenomena such as allelic bias (AB) detection, differential allelic divergence (AD) detection in case (e.g. tumor) populations compared to controls, and detection of imprinted loci with subsequent detection of loss of this imprinting (LOI) in cases. More information on these phenomena is provided throughout this vignette. As MAGE relies on population-level models, large numbers of samples are extremely beneficial (100 or more is ideal; 20 as a bare minimum).

This is an expanded vignette that doesn't utilize wrapper function, but fully writes out all analyses calling MAGE's base functions and including intermediary steps. This may not be interesting for most users that just want to use MAGE as plug-and-play (for which the regular vignette is more appropriate), but is recommended for those wanting additional insight into the pipeline or more control over its component steps for e.g. advanced analyses.

# A toy dataset

MAGE's main input consists of per-locus per-sample nucleotide counts (A/T/C/G). These are relatively straightforward to extract from BAM/SAM files via custom scripting or established pipelines such as SAMtools' mpileup or GATK's CollectAllelicCounts functionality (https://gatk.broadinstitute.org/hc/en-us/articles/360037594071-CollectAllelicCounts), though the latter already filters the data down to one reference- and one variant-allele count.

MAGE includes a small example of such data, including both a control (e.g. healthy tissue) and case (e.g. tumor tissue) dataset:

```{r}
data("MAGE", package = "MAGE")
knitr::kable(head(ControlCounts))
```

```{r}
knitr::kable(head(CaseCounts))
```

These toy datasets contain nucleotide counts for 269 loci, every locus being covered by a varying number of samples (e.g. 127 samples covering loc1 in the control dataset):

```{r, results='asis'}
length(unique(ControlCounts$locus_id))
```
```{r, results='asis'}
sum(ControlCounts$locus_id == "loc1")
```

Only a control dataset is required for RNAseq-based genotyping, imprinting detection, and allelic bias detection, but differential analyses studying e.g. the effect of disease on allelic expression (differential allelic divergence, loss of imprinting) require a comparison of cases against controls.

# Input pre-processing

MAGE expects its input data to be formatted as *lists of dataframes, every dataframe containing the per-sample nucleotide counts of one particular locus*, and has some requirements about naming as well. The first function we'll be using is `standard_alleles()`, which expects the columns `ref_alleles`, `A`, `T`, `C` and `G`. Therefore, here, the current `reference alleles` column in the toy dataset should be renamed to `ref_alleles` to comply with input requirements; each of these requirements are listed on the respective function's help pages, though besides this initial setup all remaining input requirements will be met if following this entire vignette from start to finish:

```{r}
# MAGE expects lists:
controlList <- list() 
caseList <- list()

for(n in unique(ControlCounts$locus_id)){              # For every locus...
  interDF <- ControlCounts[ControlCounts$locus_id==n,] # extract per-sample nucleotide counts
  colnames(interDF)[2] <- "ref_alleles"                # re-name the reference_allele column 
  controlList[[n]] <- interDF                          # put it into the list
}

for(n in unique(CaseCounts$locus_id)){
  interDF <- CaseCounts[CaseCounts$locus_id==n,]
  colnames(interDF)[2] <- "ref_alleles"
  caseList[[n]] <- interDF
}
```

# Determine one reference and one variant allele

MAGE employs beta-binomial modelling of allele counts, which implies only two alleles are allowed (which is compatible with the large majority of all loci in most diploid species, including humans). Accounting for more alleles would require a different approach, possibilities being the use of multiple beta-binomial models (for all alleles against one set reference, or even all pairwise combinations) or of (beta-)multinomial models”. The latter is, however, not supported by MAGE at the moment.

So as it stands, we need to select one reference and one variant allele and retain only their count data. This can be done by hand by a researcher based on biological knowledge or through some automated procedure, e.g. retaining only the most common alleles. MAGE's `standard_alleles()` function combines both approaches by taking a look at the standard alleles for every locus according to dbSNP which are supplied by the `ref_alleles` column, then selecting the two most common alleles from among these suggested standard alleles only. Should one wish to solely rely on RNAseq count files without external annotation, it's possible to supply `standard_alleles()` with `A/T/C/G` strings in all of its `ref_alleles` columns, which will result in the most abundant alleles being selected.

`standard_alleles()` updates each dataframe in our lists, retaining only two alleles in the `ref_alleles` column and including additional columns specifying which allele is chosen as reference and which as variant allele, alongside their respective counts (the reference will be the most prevalent of the two in terms of total count in all RNAseq data combined):

```{r, warning=FALSE, results = 'asis', message=FALSE}
for(n in names(controlList)){
  controlList[[n]] <- MAGE::standard_alleles(controlList[[n]])
}
knitr::kable(head(controlList[["loc27"]]))
```

We could run `standard_alleles()` on the case data as well, but to enable comparison between both datasets, we enforce selected alleles to be the same for corresponding loci in the control and case datasets (this makes sense from a biological perspective as well):

```{r}
for(n in names(caseList)){
  interDF <- caseList[[n]]
  interDF$ref_alleles <- controlList[[n]]$ref_alleles[1]
  interDF$ref <- controlList[[n]]$ref[1]
  interDF$var <- controlList[[n]]$var[1]
  interDF$ref_count <- interDF[,which(colnames(interDF)==interDF$ref[1])]
  interDF$var_count <- interDF[,which(colnames(interDF)==interDF$var[1])]
  
  caseList[[n]] <- interDF
}
```

# Prior filtering

Before doing any analyses, it might be beneficial (in terms of runtime) to filter out low-quality loci. MAGE's `prior_filter()` offers this functionality, leaving out loci based on a required minimal median coverage across all samples, a minimal number of samples, or a crudely estimated minimal minor allele frequency (just the % abundance of the variant allele over all RNAseq counts). After all, loci with a too low minor allele frequency will feature insufficient numbers of heterozygous individuals, which are required to observe ASE effects. Also, if its `checkref_filter` argument is set to TRUE, it checks whether the total nucleotide counts truly suggest there are only two occurring alleles as assumed in the previous step, or whether there's heuristic evidence to the contrary, in which case the locus is removed (details are found on this function's help page).

For now, we'll restrict ourselves to just a "minimum number of samples" filter of 20 in controls (only counting samples with at least one reference or variant read). The first analysis is genotyping after all, and researchers might be interested in getting the most likely genotype for every sample regardless of median coverage or minor allele frequency across the entire locus (as MAGE is a population-level modeller, a minimal number of samples of at least 20 is always advised). We apply no actual filtering on the case dataset however, but rather retain case data if its locus is retained in the control set as well, as comparison against controls is the case dataset's main purpose. We do run `prior_filter` on cases with no actual filter setting applied though, which reduces the function to removing samples having a zero-count for both reference and variant alleles (not removing these would give NA-results during several calculations in the pipeline). This reduces the toy datasets from 269 to 264 loci:

```{r}
for(n in names(controlList)){
  controlList[[n]] <- MAGE::prior_filter(controlList[[n]], min_median_cov = 0, 
    min_nr_samples = 20, checkref_filter = TRUE, prior_allelefreq_filter = FALSE, 
    min_PriorAlleleFreq = 0)
  caseList[[n]] <- MAGE::prior_filter(caseList[[n]], min_median_cov = 0, 
    min_nr_samples = 0, checkref_filter = FALSE, prior_allelefreq_filter = FALSE, 
    min_PriorAlleleFreq = 0)
  if(is.null(controlList[[n]])){
    caseList[[n]] <- NULL
  }
}
```

# Enabling parallellization

Before any further analyses, this vignette first splits input data into `NC` chunks below as a list, `NC` being a number of cores as the coding in this vignette allows MAGE to be run in parallel on a server using R's `parallel` package. A local (Windows) installation is only able to use 1 core at a time though, so `NC` is set as one in the code below. Feel free to change it when running MAGE on e.g. a linux-based server:

```{r}
NC <- 1 # Number of Cores
NS <- length(controlList) 
spl <- c(0, cumsum(rep(floor(NS/NC),NC)+c(rep(1,NS-floor(NS/NC)*NC),
         rep(0,NC-NS+floor(NS/NC)*NC)))) # Helps in splitting input data
ParCTRL <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParCTRL[[i]] <- controlList[(spl[i]+1):(spl[i+1])]
}
ParCASE <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParCASE[[i]] <- caseList[(spl[i]+1):(spl[i+1])]
}

```

# Metaparameter estimation

Before model fitting, we need to estimate the population-level metaparameters shared across all loci, those being the inbreeding coefficient of the population and the sequencing error rate. These values can be set based on prior knowledge about the population and the sequencing protocol of choice, but MAGE's `AllelicMeta_est()` can estimate them based on observed *control* data, which is recommended anyway for a good model fit. This function fits a regular binomial mixture model assuming no allelic bias (i.e. perfectly balanced expression of the reference- and variant allele in heterozygotes) to control read count data via expectation maximization:

$$
\small
\begin{aligned} 
\mathrm{PMF}(counts_{ref}|counts_{total}) = &\ P_{rr} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=1-SE) \ + \\
&\ P_{rv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=0.5) \ + \\
&\ P_{vv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=SE) \\
\end{aligned}
$$

With $(P_{rr}, P_{rv}, P_{vv})$ to-be-fitted genotype frequencies from which the inbreeding coefficient can be estimated, and SE a to-be-fitted sequencing error rate estimate. The function also returns a number of outputs that can be used as reliability measures, including a preliminary reference allele frequency estimate across genotypes, median coverage and number of samples. The SE-estimate itself shouldn't be absurdly high either. Default cutoff values for these quality metrics are suggested in the code below. We *don't* filter these loci out however; a suboptimal fit by `AllelicMeta_est()` may be remedied by the later beta-binomial fit allowing unequal heterozygous allele expression for actual genotyping and allelic bias detection.

As hinted upon in the previous section, the code below (and the code in following sections) puts the analysis pipelines into functions to enable parallellizations (if no parallellization is required, for-loops would do just fine). The code below returns vectors containing reliable (according to the filters mentioned above) metaparameter estimates, of which one still has to take e.g. the mean or median to get final metaparameters estimates. In this vignette, we use the more robust median.

```{r, results='asis'}
AllelicMeta_est_par <- function(DataList) {
  
  pA_filt = 0.15
  SE_filt = 0.035
  NumSamp_filt = 20
  MedianCov_filt = 4
  
  SE_vec <- c()
  F_vec <- c()
  
  positions <- names(DataList)
  results <- data.frame()
  
  for(n in names(DataList)){
    MetaEst_res <- MAGE::AllelicMeta_est(ref_counts = DataList[[n]]$ref_count, var_counts = DataList[[n]]$var_count)
    # You can store each locus' Sequencing Error (SE) and inbreeding coefficient (F) estimate 
    # in its dataframe, if you want:
    DataList[[n]]$est_SE <- MetaEst_res$SE
    DataList[[n]]$est_inbr <- MetaEst_res$F_inbr
    DataList[[n]]$allelefreq_prel <- MetaEst_res$allelefreq
    DataList[[n]]$prr_prel <- (MetaEst_res$genoprobs)$p.rr.
    DataList[[n]]$prv_prel <- (MetaEst_res$genoprobs)$p.rv.
    DataList[[n]]$pvv_prel <- (MetaEst_res$genoprobs)$p.vv.
    DataList[[n]]$genotype_prel <- MetaEst_res$genotypes
    # Only take a locus' estimates into account if the locus is high-quality:
    if (!(MetaEst_res$allelefreq <= pA_filt || MetaEst_res$allelefreq >= (1 - pA_filt) 
          # allelefreq returns allele frequency of the ref-allele,
          # so just in case this is the minor allele on population level 
          # (even though it is the most expresse one across all RNAseq data),
          # we perform the filtering like this
          || MetaEst_res$SE > SE_filt) & nrow(DataList[[n]])>=NumSamp_filt & 
        median(DataList[[n]]$ref_count + DataList[[n]]$var_count) >= MedianCov_filt) {
      SE_vec <- c(SE_vec, MetaEst_res$SE)
      F_vec <- c(F_vec, MetaEst_res$F_inbr)
    }
  }
  returnOBJ <- list("DataList_out" = DataList, "SE_vec" = SE_vec, "F_vec" = F_vec)
  return(returnOBJ)
}




cl <- parallel::makeCluster(getOption("cl.cores", NC))
GenoFinData <- parallel::parLapply(cl, X = ParCTRL, fun = AllelicMeta_est_par)
parallel::stopCluster(cl)

ParCTRL <- lapply(GenoFinData, `[[`, 1)
controlList <- do.call(c, lapply(GenoFinData, `[[`, 1))
SE_vec <- do.call(c, lapply(GenoFinData, `[[`, 2))
F_vec <- do.call(c, lapply(GenoFinData, `[[`, 3))

SEmedian <- median(SE_vec)
Fmedian <- median(F_vec)

#estimates of resp. sequencing error rate and inbreeding coefficient:
print(c(SEmedian, Fmedian))
```

These metaparameter estimates seem reasonable. Please note that an extremely low sequencing error metaparameter would make fitting of homozygous peaks later down the pipeline way too strict, failing to accomodate for even a single faulty read. As such, *setting SE lower than 0.002 is not recommended*. An inbreeding coefficient close to zero is a sign of a panmictic population and commonplace when working on human data, but it could be more extreme in specialized experimental setups.

# Genotyping RNAseq data & allelic bias detection

For both genotyping and allelic bias detection, we can use MAGE's `EMfit_betabinom_robust` function, which fits a beta-binomial mixture model using expectation maximization while allowing for unequal allelic expression in heterozygotes:

$$
\small
\begin{aligned} 
\mathrm{PMF}(counts_{ref}|counts_{total}) = &\ P_{rr} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=1-SE, \theta=\theta_{hom}) \ + \\
&\ P_{rv} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=\pi_{het}, \theta=\theta_{het}) \ + \\
&\ P_{vv} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=SE, \theta=\theta_{hom}) \\
\end{aligned}
$$

With $(P_{rr}, P_{rv}, P_{vv})$ fitted genotype frequencies, $(\theta_{hom}, \theta_{het})$ fitted overdispersion parameters in homozygotes and heterozygotes respectively, and $\pi_{het}$ the heterozygous $\pi$ parameter indicating the expected reference allele fraction, thus allowing for unequal allelic expression in heterozygotes. `pBetaBinom` is a function included in the MAGE package for the beta-binomial PMF, using a $\pi, \theta$ parameterisation (probability of success, here defined as observing a reference read, and overdispersion, respectively) instead of the classical $\alpha, \beta$ parameterisation as given on e.g. https://en.wikipedia.org/wiki/Beta-binomial_distribution. The relationship between them is $\pi = \alpha/(\alpha+\beta)$ and  $\theta = 1/(\alpha+\beta)$, resulting in a $\pi$ between 0 and 1 and a $\theta$ between 0 and infinity.

We term unequal heterozygous allele expression captured by $\pi_{het}$ *Allelic Bias (AB)*. AB can be caused by both biological (e.g. cis-eqtl loci) and technical (e.g. alignment bias) mechanisms. The fit is robust in the sense that samples with a disproportionately high impact on the heterozygous distributional parameters (determined using parameter-MLEs leaving out that particular sample) are not used for the final fit, though they are retained in the output data and genotyped using the final model (while being marked as outlier). The reason for focussing on heterozygous distributional parameters is that these are still being fitted (the homozygous $\pi$ parameters are already fixed by the metaparameter SE) and are actually affected by allele-specific expression effects ($\pi_{het}$ can indicate AB and $\theta_{het}$ can indicate allelic divergence, see later; meanwhile $\theta_{hom}$ is a nuisance parameter at best). Do note, however, that the robust fit is significantly slower than the non-robust one provided by the `EMfit_betabinom` function (proportional to the number of samples), and its merit for genotyping and AB-detection is limited, so one might opt for the latter instead. A robust fit is, however, very valuable for statistical inference on the overdispersion parameter $\theta$ (see the next section).

MAGE's functions have many optional input parameters to customize the fitting procedure, but throughout this tutorial and for most applications the defaults should suffice. Feel free to read up on all options on each function's help page. After performing the fit and writing results of interest into dataframes, we also perform a chi-square test assessing Hardy-Weinberg Equilibrium using our final genotypes and the previously determined inbreeding coefficient hyperparameter, to be used for filtering later.

```{r}
BetaBinomGenotyping <- function(data){
  positions <- names(data)
  results <- data.frame()
  for (z in positions) {
    MAGEres <- MAGE::EMfit_betabinom_robust(data_counts = data[[z]], 
                                            SE = SEmedian, inbr = Fmedian)
    data[[z]] <- MAGEres$data_hash
    # median_AB also calculates a robust median AB,
    # besides the AB as determined during EMfit_betabinom_robust's fitting procedure.
    # This can be used as additional filter when detecting significant AB:
    med_AB <- MAGE::median_AB(data[[z]]$ref_count, data[[z]]$var_count, 
                               data[[z]]$allelefreq[1], Fmedian) 
    res_loc <- data.frame("position" = z, "probshift" = as.numeric(MAGEres$AB), 
      "LRT" = as.numeric(MAGEres$AB_lrt), "p" = as.numeric(MAGEres$AB_p),
      "quality" = MAGEres$quality, "allele.frequency" = data[[z]]$allelefreq[1], 
      "reference" = data[[z]]$ref[1], "variant" = data[[z]]$var[1], 
      "est_SE" = data[[z]]$est_SE[1], "coverage" =  median(data[[z]]$ref_count+data[[z]]$var_count),
      "nr_samples" = nrow(data[[z]]), "median_AB" = med_AB, 
      "rho_rr" = MAGEres$rho_rr, "rho_rv" = MAGEres$rho_rv, "rho_vv" = MAGEres$rho_vv, 
      "theta_hom" = MAGEres$theta_hom, "theta_het" = MAGEres$theta_het,
      "theta_hom_NoShift" = MAGEres$theta_hom_NoShift, 
      "theta_het_NoShift" = MAGEres$theta_het_NoShift, stringsAsFactors = FALSE)
    
    results <- rbind(results, res_loc) # results; one position per line
  }
  results <- MAGE::HWE_chisquared(data = data, Fmedian, results = results)
  results$Chi2PVAL[is.na(results$Chi2PVAL)] <- 
  results$Chi2STAT[is.na(results$Chi2STAT)] <- -1
  return(list(data, results))
}

cl <- parallel::makeCluster(getOption("cl.cores", NC))
parallel::clusterExport(cl, c("Fmedian", "SEmedian"))
GenoFinData <- parallel::parLapply(cl, X = ParCTRL, fun = BetaBinomGenotyping)
parallel::stopCluster(cl)

ParCTRL <- lapply(GenoFinData, `[[`, 1)
controlList <- do.call(c, lapply(GenoFinData, `[[`, 1))
Geno_AB_res <- do.call("rbind", lapply(GenoFinData, `[[`, 2))
```

As a sidenote, while this vignette represses warnings, calling upon MAGE's beta-binomial density function (`pBetaBinom`, which `EMfit_betabinom_robust` does) can result in a memory limit warning which can usually be ignored; most of the time it signifies that a certain locus' beta-binomial fit was so close to a regular binomial fit that the overdispersion parameter is extremely close to zero, leading to precision errors in beta-binomial calculations which will trigger MAGE to use regular binomial functions instead.

The genotyping results can now be retrieved from the `controlList` object, being included as the "genotypeN" column in each locus' dataframe:

```{r}
knitr::kable(head(controlList[["loc27"]][,c("locus_id", "sample_id", "ref", 
                                            "var", "genotypeN")]))
```

The `Geno_AB_res` dataframe contains the results of AB-detection, i.e. testing whether the reference allele expression ratio differs from a "perfectly balanced" ratio of 0.5 (i.e. is there a preference to detect one of both alleles?). The "probshift" column contains the actual fitted allelic ratio while the "p" column contains the p-value of a likelihood-ratio-test assessing significant deviance from 0.5; both can be filtered on to retain only significantly allelically biased loci with a big enough effect size. 

Other columns contain additional filter criteria, like

* "median_AB": a median (robust) reference allele ratio estimate in case you want to play it extra safe in retaining significantly biased loci
* "quality": will contain "!" if a locus contained no heterozygous samples to detect AB on
* "allele.frequency": the reference allele frequency across genotypes; you may not want to trust loci with extreme allele frequencies as these probably contain very little heterozygous individuals
* "coverage": contains the median coverage of all samples covering the locus
* "nr_samples": number of samples covering the locus

The code below first retains loci showcasing statistically significant AB (controlling the false discovery rate at 0.05) which are of good quality. Then, we also construct a more high-fidelity set of loci, retaining only those of good quality with a fitted allelic ratio greater than 0.6 or smaller than 0.4, yet not more extreme than 0.9 or 0.1 because those could be the product of poorly fitted loci. The median ratio should also be larger than 0.6 or smaller than 0.4, allele frequency should not be more extreme then 0.9 or 0.1, median coverage should be over 10 and number of samples should be over 80. These are reasonable filter criteria, though their specific values depend on each end user's desires en the specific dataset used (e.g. requiring over 80 samples is perhaps too stringent for smaller datasets and too liberal for larger ones).

In BOTH CASES, we *only retain loci that are in Hardy-Weinberg Equilibrium (HWE)* according to the chi square test performed earlier. Since we retain only those loci for which we can't find sufficient evidence against the null hypothesis of HWE, this is a statistically weak conclusion, yet in scientific research a cut-off of the p-value of this test of 0.001 is often used to filter out loci with which something seems wrong (see e.g. @sha, @teo, @rohlfs); such extreme deviations from HWE should not occur in a normal population.

```{r, results='asis'}
# Statistical evidence for significant AB at the 5% FDR level:
print(paste(Geno_AB_res$position[p.adjust(Geno_AB_res$p, method = "BH") < 0.05 & 
      Geno_AB_res$quality != "!" & Geno_AB_res$Chi2PVAL >= 0.001], collapse = ", "))

```

```{r, results='asis'}
# Statistical evidence for significant AB at the 5% FDR level,
# only retaining reliable (high-quality) loci with a large enough effect size:
print(paste(Geno_AB_res$position[p.adjust(Geno_AB_res$p, method = "BH") < 0.05 & 
  Geno_AB_res$quality != "!" & (Geno_AB_res$probshift > 0.6 | Geno_AB_res$probshift < 0.4) 
  & Geno_AB_res$probshift < 0.9 &  Geno_AB_res$probshift > 0.1 & (Geno_AB_res$median_AB > 0.6 | 
  Geno_AB_res$median_AB < 0.4) & Geno_AB_res$allele.frequency < 0.9 & 
  Geno_AB_res$allele.frequency > 0.2 & Geno_AB_res$coverage > 10 & 
  Geno_AB_res$nr_samples > 80 & Geno_AB_res$Chi2PVAL >= 0.001], collapse = ", "))

```

MAGE contains a function to plot histograms of the reference allele fraction across samples for a specific locus, while also including PMF plots of both the unshifted (assuming no AB) and shifted models. The theta-parameters for the unshifted model are included in the previous `EMfit_betabinom_robust()` output as they are calculated anyway to test for a significant shift using a likelihood ratio test. This plot can illustrate just how much better the shifted model fits to the data, as illustrated below. 

Keep in mind, however, that for a beta-binomial PMF, its shape (variance) depends on the coverage, even when normalized from the reference allele count to a 0-to-1 reference allele fraction, such as on the x-axis in the plot below. As such, we have to choose a coverage to use as basis for the PMF plots (`ScaleCount` argument). The median coverage across samples is a good choice for this, though for data points with outlying coverages, the resulting visualization may still be poor (even if those data points are well fitted by the model). This is because the fitted model allows for a flexible per-sample coverage parameter, whereas for our PMF plot, we have to choose just one.

To accomodate for this, one might opt to set the `ScaleHist` argument to TRUE, which will re-calculate observed per-sample reference allele fractions that are to be plotted in the histogram using the MAGE's beta-binomial quantile function given the observed per-sample data probability as input, but while using `ScaleCount` total counts (i.e. we transform observed reference allele fractions to the fractions we "would have observed" - i.e. with the same cumulative distribution function value - given `ScaleCount` observed reads, assuming the fitted beta-binomial model is correct). It's up to the end user to use this argument, though for a comfortable interpretation of the plots it's recommended if the per-sample coverage is very variable.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
PlotData <- controlList[["loc11"]]
PlotData_eqtl <- Geno_AB_res[Geno_AB_res$position=="loc11",]

loc_plot <- MAGE::MAGE_EMfitplot(ref_counts=PlotData$ref_count, 
  var_counts=PlotData$var_count, pr=PlotData_eqtl$rho_rr, prv=PlotData_eqtl$rho_rv,
  pv=PlotData_eqtl$rho_vv, theta_hom=PlotData_eqtl$theta_hom, 
  theta_het=PlotData_eqtl$theta_het, pr_NoShift=mean(PlotData$prr_H0), 
  prv_NoShift=mean(PlotData$prv_H0), pv_NoShift=mean(PlotData$pvv_H0), 
  theta_hom_NoShift=PlotData_eqtl$theta_hom_NoShift, 
  theta_het_NoShift=PlotData_eqtl$theta_het_NoShift, 
  probshift=as.numeric(PlotData_eqtl$probshift), SE=SEmedian, 
  ScaleCount = PlotData_eqtl$coverage, ScaleHist = TRUE, plot_NoShift = TRUE, nbins = 30)

loc_plot
```

The plotting function contains some customization options, the plot above mainly using default colors with the PMF of the shifted fit split up according to genotype. One can e.g. choose to plot one combined unshifted PMF, and to not plot the unshifted fit, via the code below (once again using default colors, though these can all be changed):

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
loc_plot <- MAGE::MAGE_EMfitplot(ref_counts=PlotData$ref_count, 
  var_counts=PlotData$var_count, pr=PlotData_eqtl$rho_rr, prv=PlotData_eqtl$rho_rv,
  pv=PlotData_eqtl$rho_vv, theta_hom=PlotData_eqtl$theta_hom, 
  theta_het=PlotData_eqtl$theta_het, pr_NoShift=mean(PlotData$prr_H0), 
  prv_NoShift=mean(PlotData$prv_H0), pv_NoShift=mean(PlotData$pvv_H0), 
  theta_hom_NoShift=PlotData_eqtl$theta_hom_NoShift,
  theta_het_NoShift=PlotData_eqtl$theta_het_NoShift, 
  probshift=as.numeric(PlotData_eqtl$probshift), SE=SEmedian, 
  ScaleCount = PlotData_eqtl$coverage, ScaleHist = TRUE, plot_NoShift = FALSE,
  SplitPeaks = FALSE, nbins = 30)

loc_plot
```

# Differential allelic divergence detection

In the context of MAGE, we see case-specific events such copy number alterations, aberrant hypermethylation, promotor mutation and silencing events of e.g. tumor suppressor genes as mechanisms that will affect one allele, or both alleles in a different way, per sample. However, these mechanisms show no preference towards either allele on a population-level, acting mainly as an added source of variance on the observed reference allele fraction captured by $\theta_{het}$ while leaving $\pi_{het}$ unchanged. Calling this variance *Allelic Divergence (AD)*, we term its change (increase) in disease due to, amongst others, the mechanisms listed above *differential Allelic Divergence (dAD)*.

Detecting dAD between two populations can be done via a likelihood ratio test, comparing: (1) a model fitting ALL data, both control and case, as one beta-binomial mixture model, with (2) a model allowing the control- and case data to have their separate $\theta$ parameters for the heterozygous samples, yet keeping the $\pi$ parameter constant. These two models' parameters are distinguished by respectively `H0` (null hypothesis) and `H1` (alternative hypothesis) in the code below. The pivotal new function is `EMfit_betabinom_popcomb` which fits the model assuming different heterozygote-thetas while keeping all other parameters shared (i.e. the `H1` model):

```{r warning=FALSE}
ParCASE <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParCASE[[i]] <- caseList[(spl[i]+1):(spl[i+1])]
}
ParTOT <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParTOT[[i]] <- list(ParCTRL[[i]], ParCASE[[i]])
}

dAD_analysis <- function(data){
  
  controlListP <- data[[1]]
  caseListP <- data[[2]]
  positions <- names(controlListP)
  
  dAD_res <- data.frame(LocName = names(controlListP), PiFitH0 = 0, PiFitH1 = 0, ThetaHetH0 = 0,
    ThetaHetCTRL = 0, ThetaHetCASE = 0, RhoHetH0 = 0, RhoHetCTRL = 0, RhoHetCASE = 0, LRTpval = 0, 
    NumHetCTRL = 0, NumHetCASE = 0, RobFlagCTRL = "", RobFlagCASE = "", HWECTRL = 0,
    HWECASE = 0, CovCTRL_mean = 0, CovCASE_mean = 0, CovCTRL_med = 0, CovCASE_med = 0,
    NumOutCTRL = 0, NumOutCASE = 0, QualityCTRL = "N", QualityCASE = "N", pr = 0, 
    prv = 0, pv = 0, ThetaHomH0 = 0, ThetaHomH1 = 0)

  for (LOC in positions) {
    
    CTRL_DF <- data.frame("ref_count" = controlListP[[LOC]]$ref_count, 
                       "var_count" = controlListP[[LOC]]$var_count, "isCase" = 0)
    CASE_DF <- data.frame("ref_count" = caseListP[[LOC]]$ref_count, 
                       "var_count" = caseListP[[LOC]]$var_count, "isCase" = 1)
    
    # Previously, we used EMfit_betabinom_robust() to ROBUSTLY fit our models by removing
    # outliers via Cook's distance. Now however, we're working with data from two different
    # sources (control- and case tissue) and have no way of knowing in advance how similar
    # these are. As such, outlier detections should happen ON BOTH SETS SEPARATELY yet the
    # two hypotheses we'll be fitting share all or some parameters between the two. 
    # For this, we can use the EMfit_betabinom_robust() function with its "fitH0" set to 
    # FALSE, which will not complete the entire eqtl-detection pipeline, but will cut it 
    # short before fitting the unshifted model. By running this function on both our 
    # datasets separately in advance of the dAD-relevant fits, we ensure correct outlier
    # detection ánd the use of the same dataset in our upcoming likelihood ratio tests.
  
    # 1. Detect and extract outliers using EMfit_betabinom_robust()
    OUTfitCTRL <- MAGE::EMfit_betabinom_robust(data_counts = CTRL_DF, SE = SEmedian, 
                                               inbr = Fmedian, fitH0 = FALSE)
    OUTfitCASE <- MAGE::EMfit_betabinom_robust(data_counts = CASE_DF, SE = SEmedian,
                                               inbr = Fmedian, fitH0 = FALSE)
    OUTfitCTRL_DH <- OUTfitCTRL$data_hash; OUTfitCASE_DH <- OUTfitCASE$data_hash
    CTRL_DF$Outlier <- OUTfitCTRL_DH$Outlier; CASE_DF$Outlier <- OUTfitCASE_DH$Outlier
    CurDF <- rbind(CTRL_DF, CASE_DF)
    # These results are also handy for an estimation of the number of heterozygotes in 
    # controls and tumors AND the number of outliers, both good filter criteria.
    # We can also include a "RobFlag" which gives more information about outlier detection 
    # (e.g. none detected, or so unreasonably many that none were removed)
    dAD_res$NumHetCTRL[dAD_res$LocName == LOC] <- sum(OUTfitCTRL_DH$prv)
    dAD_res$NumHetCASE[dAD_res$LocName == LOC] <- sum(OUTfitCASE_DH$prv)
    dAD_res$NumOutCTRL[dAD_res$LocName == LOC] <- sum(CTRL_DF$Outlier)
    dAD_res$NumOutCASE[dAD_res$LocName == LOC] <- sum(CASE_DF$Outlier)
    dAD_res$RobFlagCTRL[dAD_res$LocName == LOC] <- OUTfitCTRL$RobFlag
    dAD_res$RobFlagCASE[dAD_res$LocName == LOC] <- OUTfitCASE$RobFlag
  
  
    # 2. Perform one fit on the entire (non-outlying) data, i.e. all parameters shared,
    # i.e. assuming no dAD; the null hypothesis in dAD detection.
    # Remark this uses the non-robust fitting function, since outliers were already detected.
    NOdAD_fit <- MAGE::EMfit_betabinom(data_counts = CurDF[CurDF$Outlier == 0,], 
      SE = SEmedian, inbr = Fmedian, fitH0 = FALSE)
    NOdAD_fit_DF <- NOdAD_fit$data_hash
    PiH0 <- NOdAD_fit$AB
    rho_rr <- NOdAD_fit$rho_rr; rho_rv <- NOdAD_fit$rho_rv; rho_vv <- NOdAD_fit$rho_vv
    ThetaHomH0 <- NOdAD_fit$theta_hom; ThetaHetH0 <- NOdAD_fit$theta_het
  
    dAD_res$PiFitH0[dAD_res$LocName == LOC] <- PiH0
    dAD_res$ThetaHetH0[dAD_res$LocName == LOC] <- ThetaHetH0
  
  
    # 3. Perform a fit on the (non-outlying) data allowing separate theta_het parameters 
    # for control- and case-data
    FullFit <- MAGE::EMfit_betabinom_popcomb(data_counts = CurDF[CurDF$Outlier == 0,],
      SE = SEmedian, inbr = Fmedian, probshift_init = PiH0)
    ParamVec <- FullFit$ParamVec
    dAD_res$PiFitH1[dAD_res$LocName == LOC] <- ParamVec["probshift"]
    dAD_res$ThetaHetCTRL[dAD_res$LocName == LOC] <- ParamVec["theta_het_control"]
    dAD_res$ThetaHetCASE[dAD_res$LocName == LOC] <- ParamVec["theta_het_case"]
    
  
    # 4. Perform the Likelihood Ratio Test for dAD detection
    # Likelihood of fit with all parameters shared:
    LikTot <- MAGE::pmf_betabinomMix(CurDF[CurDF$Outlier==0,]$ref_count, 
      CurDF[CurDF$Outlier==0,]$var_count, probshift = PiH0, SEmedian, rho_rr, rho_vv, rho_rv, 
      theta_hom = ThetaHomH0, theta_het = ThetaHetH0)
    # Likelihood of fit with separate theta_het (calculated in two steps because of the 
    # different theta)
    LikCTRL <- MAGE::pmf_betabinomMix(CTRL_DF[CTRL_DF$Outlier==0,]$ref_count, 
      CTRL_DF[CTRL_DF$Outlier==0,]$var_count, probshift = ParamVec["probshift"], SEmedian,
      ParamVec["pr"], ParamVec["pv"], ParamVec["prv"], theta_hom = ParamVec["theta_hom"],
      theta_het = ParamVec["theta_het_control"])
    LikCASE <- MAGE::pmf_betabinomMix(CASE_DF[CASE_DF$Outlier==0,]$ref_count, 
      CASE_DF[CASE_DF$Outlier==0,]$var_count, probshift = ParamVec["probshift"], SEmedian,
      ParamVec["pr"], ParamVec["pv"], ParamVec["prv"], theta_hom = ParamVec["theta_hom"], 
      theta_het = ParamVec["theta_het_case"])
    lrtstat <- -2 * (sum(log(LikTot)) - sum(log(c(LikCTRL, LikCASE))))
    LRTpval <- pchisq(lrtstat, df = 1, lower.tail = F)
    dAD_res$LRTpval[dAD_res$LocName == LOC] <- LRTpval
  
  
    # 5. Fill out the results dataframe
    dAD_res$QualityCTRL[dAD_res$LocName == LOC] <- # spot bad quality data
      OUTfitCTRL$quality; dAD_res$QualityCASE[dAD_res$LocName == LOC] <-OUTfitCASE$quality 
    # Test HWE on both the control and tumor data:
    HWEtest_CTRL <- MAGE::HWE_chisquared(inbr = Fmedian, data = OUTfitCTRL_DH)
    dAD_res$HWECTRL[dAD_res$LocName == LOC] <- HWEtest_CTRL$PVAL
    HWEtest_CASE <- MAGE::HWE_chisquared(inbr = Fmedian, data = OUTfitCASE_DH)
    dAD_res$HWECASE[dAD_res$LocName == LOC] <- HWEtest_CASE$PVAL
    # Mean and median coverages:
    dAD_res$CovCTRL_mean[dAD_res$LocName == LOC] <- 
      mean(CTRL_DF[CTRL_DF$Outlier==0,]$ref_count + CTRL_DF[CTRL_DF$Outlier==0,]$var_count)
    dAD_res$CovCASE_mean[dAD_res$LocName == LOC] <- 
      mean(CASE_DF[CASE_DF$Outlier==0,]$ref_count + CASE_DF[CASE_DF$Outlier==0,]$var_count)
    dAD_res$CovCTRL_med[dAD_res$LocName == LOC] <- 
      median(CTRL_DF[CTRL_DF$Outlier==0,]$ref_count + CTRL_DF[CTRL_DF$Outlier==0,]$var_count)
    dAD_res$CovCASE_med[dAD_res$LocName == LOC] <- 
      median(CASE_DF[CASE_DF$Outlier==0,]$ref_count + CASE_DF[CASE_DF$Outlier==0,]$var_count)
    dAD_res$pr[dAD_res$LocName == LOC] <- ParamVec["pr"]
    dAD_res$prv[dAD_res$LocName == LOC] <- ParamVec["prv"]
    dAD_res$pv[dAD_res$LocName == LOC] <- ParamVec["pv"]
  
    dAD_res$ThetaHomH0[dAD_res$LocName == LOC] <- ThetaHomH0
    dAD_res$ThetaHomH1[dAD_res$LocName == LOC] <- ParamVec["theta_hom"]
  }

  dAD_res$HWECTRL[is.na(dAD_res$HWECTRL)] <- 
  dAD_res$HWECTRL[is.na(dAD_res$HWECTRL)] <- -1
  
  dAD_res$RhoHetH0 <- 1/((1/dAD_res$ThetaHetH0)+1)
  dAD_res$RhoHetCTRL <- 1/((1/dAD_res$ThetaHetCTRL)+1)
  dAD_res$RhoHetCASE <- 1/((1/dAD_res$ThetaHetCASE)+1)
    
  return(dAD_res)
}

cl <- parallel::makeCluster(getOption("cl.cores", NC))
parallel::clusterExport(cl, c("Fmedian", "SEmedian"))
dADFinData <- parallel::parLapply(cl, X = ParTOT, fun = dAD_analysis)
parallel::stopCluster(cl)

dAD_res <- do.call("rbind", dADFinData)
```

In `dAD_res`, both `theta`- and `rho`-values are listed, which both represent the overdispersion parameter; $\theta$ just has a range from 0 to infinity while $\rho$ has been rescaled to go from 0 to 1, which is more convenient for some analyses, such as plotting. When studying differential allelic divergence, one usually tries to look for "disturbed regions" by plotting the results in `dAD_res` among the length of chromosomes (e.g. plotting tumor rho-values along the chromosomes while also marking the actually statistically relevant loci in some way), as will be demonstrated at the end of this section.

Note that both parameters are perfectly equivalent, and that the exact parametrization does not impact the statistical testing procedure (as both will yield exactly the same likelihoods).

That being said, you can also just extract loci showing differential allelic divergence from `dAD_res` using the `LRTpval` column to retain statistically significant results. Defining an effect size is a bit trickier; you'd expect to be able to do it on the overdispersion-parameter-values of cases compared to controls, but how would one go about it? When both are very small (e.g. rho ~ 10^-7 range), one being 10 times as large as the other doesn't mean that much, but when both are quite large (e.g. rho ~ 10^-2 range) this does make a very large difference. Subtracting one from the other has an equally unclear interpretation regarding effect size. The effect of the overdispersion parameter on the actual variance also depends on the pi parameter, which further complicates things. Moreover, in many cases, e.g. cancer, case samples will be mixtures of normal and diseased cells, with varying (gene-dependent) levels of expression in each cell type, further complicating the creation of appropriate effect sizes.

This type of difficulties made us opt for detecting affected regions on plots rather than relying on the effect sizes as such. But if you wanted to e.g. only retain loci that show a strong deviation from a regular binomial regarding its variance, using a rho-cutoff of, say, 0.1, can do. Such cutoffs are based on experience and can be played around with. In the code below, we retain such good-quality loci which are also statistically significant regarding dAD and have a median coverage of at least 10 in both controls and cases, as well as at least 15 heterozygous samples and data conform Hardy-Weinberg Equilibrium in controls only (reliable case data genotyping may be difficult when faced with extreme differential allelic divergence). Also, note that for genuine differential allelic divergence, overdispersion will be typically be larger in cases than in controls.

```{r, results='asis'}
print(paste(dAD_res$LocName[dAD_res$RhoHetCASE >= 0.1 & p.adjust(dAD_res$LRTpval, 
  method = "BH") < 0.05 & dAD_res$CovCTRL_med >= 10 & dAD_res$CovCASE_med >= 10 & 
  dAD_res$NumHetCTRL >= 15 & dAD_res$HWECTRL >= 0.001 & 
  dAD_res$RhoHetCASE > dAD_res$RhoHetCTRL], collapse = ", "))
```

The previously used `MAGE_EMfitplot` function can also be used to vizualize the dAD detection results via separate plots of the control- and case-population utilizing their separately fitted overdispersion parameters. Here, it's important to set the input argument `ScaleCount` to the same value for both plots, so the width of the PMFs can actually be compared correctly.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
CTRL_DF <- controlList[["loc85"]]
CASE_DF <- caseList[["loc85"]]
PlotData_dAD <- dAD_res[dAD_res$LocName=="loc85",]

dAD_plot1 <- MAGE::MAGE_EMfitplot(ref_counts=CTRL_DF$ref_count, var_counts=CTRL_DF$var_count,
  pr=PlotData_dAD$pr, prv=PlotData_dAD$prv, pv=PlotData_dAD$pv, 
  theta_hom = PlotData_dAD$ThetaHomH1, theta_het = PlotData_dAD$ThetaHetCTRL, 
  probshift = PlotData_dAD$PiFitH1, SE = SEmedian, ScaleCount = 50, ScaleHist = TRUE, 
  nbins = 30, plot_NoShift = FALSE)

dAD_plot2 <- MAGE::MAGE_EMfitplot(CASE_DF$ref_count, CASE_DF$var_count, pr=PlotData_dAD$pr,
  prv=PlotData_dAD$prv, pv=PlotData_dAD$pv, theta_hom = PlotData_dAD$ThetaHomH1, 
  theta_het = PlotData_dAD$ThetaHetCASE, probshift = PlotData_dAD$PiFitH1, SE = SEmedian,
  ScaleCount = 50, ScaleHist = TRUE, nbins = 30, plot_NoShift = FALSE)

gridExtra::grid.arrange(dAD_plot1, dAD_plot2, ncol=2)
```

The plot above is of a locus showcasing very clear differential allelic divergence, with the reference allele fraction in heterozygous case samples being way more variable, probably due to case-specific effects.

As promised, to conclude, this section showcases a plot of dAD results across a chromosome using the `MAGE_ADChromplot` function. The toy data isn't very fit for this, however (no chromosomal location, too small), so included with the toy data is some thoroughly processed data to illustrate the plot on. The inputs needed are $\rho$ values in both controls and cases along with differential expression data (log2 fold changes) which can be easily generated by tools for differential expression analysis on the data at hand. `MAGE_ADChromplot` optionally even accepts additional hypermethylation and copy-number-alteration data for even more extensive plots. As this is for illustrative purposes only, we just make the plot using the provided datasets below, but more details can be learned from `MAGE_ADChromplot`'s help page. The function returns several plot components (including legends) as separate objects due to its complexity, so it's up to the end user to arrange them into a satisfactory composition.

As a final remark, many of the measures in this plot (differential expression, hypermethylation, copy number alterations) are usually obtained at the gene-level rather than the SNP/locus level. To make the results of different analyses more compareable, MAGE's per-locus p-values can be combined using the `combine_p_gene` function. For this, we recommend using the geometric mean as a nice balance between more conservative (arithmetic mean, maximum) and liberal (harmonic mean, minimum) methods for combining p-values, and the different loci should be weighted as well, for which we recommend $\small \sqrt{median \ locus \ coverage \ * \ estimated \ number \ of \ heterozygous \ samples }$, the latter being an output of MAGE's beta-binomial mixture model fits via the heterozygote genotype frequency times the number of samples. For p-values concerning a comparison between a control- and case-population, one can e.g. use the minimum of this weight across populations.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
ChromPlot <- MAGE::MAGE_ADChromplot(AD_Data, DE_Data, Meth_Data, CNAgain_Data, CNAloss_Data,
                                    pvalSIG = 0.05, roll_median = 15)

ChromPlot[["ADDE_plot"]] / ChromPlot[["MethCNA_plot"]] / (ChromPlot[["LEG1"]] + 
  ChromPlot[["LEG2"]] + ChromPlot[["LEG3"]]) + 
  patchwork::plot_layout(heights = c(2,1,0.5), widths = c(1,1,1))
```

The histograms in the top panes show (rolling medians) of the overdisperstion ($\rho$) parameters in controls (black) and cases (colored). The case-$\rho$ is colored according to the (already 5% FDR corrected) p-value testing for significant dAD in cases versus controls, while the difference in height of these histograms provide an illustration of the effect size (actual $\rho$ values). The remaining plot elements are explained in the legend.

# Detecting imprinted and differentially imprinted loci

Imprinting analyses in MAGE are quite different from the analyses up until now in that they don't rely on beta-binomial modelling and make different assumptions about the data. They are, generally speaking, a bit more robust because imprinting is usually an extreme phenomenon, i.e. heterozygous samples of imprinted loci will almost always "seem like" homozygous samples because one of the alleles has been silenced almost entirely. Partial imprinting can exist and MAGE can detect it, but this phenomenon is not modelled as thoroughly as the previous ones in this vignette.

First off we assume samples show no AB and are conform Hardy-Weinberg-Equilibrium while taking imprinting into account, which is tested by `symmetry_gof`. It does this by assessing whether the number of samples with a reference allele fraction > 0.5 (and so also the number with a fraction <= 0.5) is equal to that expected under HWE assuming no AB using a chi square test. This may seem kind of heuristic at first and prone to fail if there is even a little AB, but remember we want to mainly retain imprinted loci, meaning heterozygous samples should behave as one of both homozygotes (and so clearly feature reference allele fractions > 0.5 or <=0.5) so this filter is very fitting for retaining HWE-conform samples that are at the same time likely to showcase imprinting. A good filter setting is requiring the p-value to be at least 0.05.

After applying these filters, `imprinting_est` detects imprinted loci in the remaining control samples by first assuming an unshifted binomial mixture model, then splitting up the heterozygous samples in two separate groups according to a degree of imprinting $i$ and varying this $i$ from 0 to 1 in a for-loop, calculating likelihoods along the way to perform a likelihood-ratio test to check for differential imprinting (i.e. compare the most likely model with the non-imprinted one; $i=0$). For completeness, the imprinted PMF is (see @goovaerts2018 for a full elaboration):

$$
\small
\begin{aligned} 
\mathrm{PMF}(&counts_{ref}|counts_{total}) =  P_{rr} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=1-SE) \ + \\
& 0.5P_{rv} * {\tt pbinom} \left( x=counts_{ref} \ |\ n=counts_{total}, p=\frac{0.5-\frac{i}{2}}{1-\frac{i}{2}}(1-SE)+\frac{0.5}{1-\frac{i}{2}}SE\right) \ + \\
& 0.5P_{rv} * {\tt pbinom} \left( x=counts_{ref} \ |\ n=counts_{total}, p=\frac{0.5-\frac{i}{2}}{1-\frac{i}{2}}SE+\frac{0.5}{1-\frac{i}{2}}(1-SE)\right) \ + \\
& P_{vv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=SE)
\end{aligned}
$$

After retaining only significantly and sufficiently imprinted loci (5% FDR level, degree of imprinting at least 0.6 and median degree of imprinting at least 0.8 for robustness; see `median_imprinting` for the latter's calculation) using the `final_filter` function, we use `LOItest_logreg()` to test for differential imprinting. More specifically, logistic regression is used to evaluate whether case samples feature a significantly higher number of apparently heterozygous samples than control samples (by considering the least expressed allele as a success; more details can be found on the function's help page. Note that the logistic regression approach takes into account variable coverage between samples, contrasting our previously published strategy by @goovaerts2018). If this is true, it indicates that a significant fraction of heterozygous samples in tumor tissue "lost their imprinting" and actually started expressing both alleles again. We call these loci “differentially imprinted”, as we reserve the term “loss-of-imprinting” for the phenomenon where differential imprinting co-occurs with increased total expression, indicative of re-expression of the silenced allele. Comments in the code below provide additional insight into the imprinting analysis pipeline:

```{r warning=FALSE}
# Perform filtering using symmetry_gof()
# Notice we use allelefreq_prel as input in this function, which is the allele frequency as 
# estimated by an UNSHIFTED binomial mixture model using AllelicMeta_est() earlier in this 
# vignette, which is what symmetry_GOF assumes as well so it's only fitting.
# We also enforce that this allelefreq_prel can not be more extreme than 0.15 or 0.85, 
# because detecting imprinting would be very hard otherwise.
ImprData <- controlList
for(LOC in names(controlList)){
  if (ImprData[[LOC]]$allelefreq_prel[1] <= 0.15 || 
      ImprData[[LOC]]$allelefreq_prel[1] >= (1 - 0.15)) {
    ImprData[[LOC]] <- NULL
  } else {
    ImprData[[LOC]]$sym <- MAGE::symmetry_gof(ImprData[[LOC]]$ref_count, 
      ImprData[[LOC]]$var_count, ImprData[[LOC]]$allelefreq_prel[1])
    if (ImprData[[LOC]]$sym[1] <= 0.05) {
      ImprData[[LOC]] <- NULL
    }
  }
}


# Detect imprinted control loci
impr_res <- data.frame()
for(LOC in names(ImprData)){
  i_results <- MAGE::imprinting_est(ImprData[[LOC]]$ref_count, ImprData[[LOC]]$var_count, 
                allelefreq = ImprData[[LOC]]$allelefreq_prel[1], SE = SEmedian, inbr = Fmedian)
  # An additional robustified "median imprinting" across samples to be used as possible 
  # additional filter criterion:
  med_imp <- MAGE::median_imprinting(ImprData[[LOC]]$ref_count, ImprData[[LOC]]$var_count, 
              allelefreq = ImprData[[LOC]]$allelefreq_prel[1], inbr = Fmedian)
  results_z <- data.frame("position" = ImprData[[LOC]]$locus_id[1], "LRT" = i_results$LRT, 
    "p" = i_results$p_value, "estimated.i" = i_results$est_i, "allele.frequency" = 
    ImprData[[LOC]]$allelefreq_prel[1], "reference" = ImprData[[LOC]]$ref[1], "variant" = 
    ImprData[[LOC]]$var[1], "med_cov" = ImprData[[LOC]]$coverage[1], "nr_samples" = 
    nrow(ImprData[[LOC]]), "GOF" = i_results$GOF_likelihood, "symmetry" = 
    ImprData[[LOC]]$sym[1], "med_impr" = med_imp, stringsAsFactors = FALSE)
  impr_res <- rbind(impr_res, results_z)
}
# Retain significantly imprinted loci (5% FDR) utilizing some additional filters, amongst
# which a custom  Goodness-Of-Fit which more or less corresponds to a locus' likelihood of
# the imprinted model*coverage; 0.8 is a good cutoff. Other filter criteria are imprinting 
# (0.6) and median imprinting (0.8)
impr_res_FIN <- MAGE::final_filter(data_hash=NULL, impr_res, results_wd=NULL, gof_filt = 0.8, 
  med_impr_filt = 0.8, i_filt = 0.6, adj_p_filt = 0.05, file_all = FALSE, file_impr = FALSE, 
  file_all_counts = FALSE, file_impr_counts = FALSE)


# Amongst actually imprinted loci, detect differential expression in case data
pos_impr <- as.character(impr_res_FIN$position)
p_DI_df <- impr_res_FIN
p_DI_df$DI_pval <- 1
for(LOC in pos_impr){
  CData <- controlList[[LOC]]
  TData <- caseList[[LOC]]
  p_DI <- MAGE::LOItest_logreg(CData$ref_count, CData$var_count, 
                                  TData$ref_count, TData$var_count)$p.value
  p_DI_df$DI_pval[p_DI_df$position == LOC] <- p_DI
}

```

We can take a look at the loci that are significantly and sufficiently imprinted (i.e. the output of `final_filter` from above):

```{r, results='asis'}
print(paste(impr_res_FIN$position, collapse = ", "))
```

From this set, a part is actually differentially imprinted in case samples, which is indicated by the p-value in the `DI_pval` column in the `p_DI_df` dataframe. We can control loss-of-imprinting detection, for a change, at the 5% FWER level (actually feasible here due to the small number of actually imprinted loci remaining, though we're looking at a small toy dataset here; this isn't necessarily the case in larger experiments):

```{r, results='asis'}
print(paste(p_DI_df$position[p.adjust(p_DI_df$DI_pval, method = "holm") < 0.05], 
            collapse = ", "))
```

We can plot histograms of both the control and case allele fractions; locus 202 is obviously imprinted in controls and loses a lot of this imprinting in cases (controls are plotted using `MAGE_imprintplot` to visualize imprinting results, but since an imprinting-estimation never happens on the case data - we just detect a change in heterozygosity via logistic regression - we simply make a histogram for cases instead).

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
HistCTRL <- MAGE::MAGE_imprintplot(controlList[["loc202"]]$ref_count, 
  controlList[["loc202"]]$var_count, 
  allelefreq = impr_res_FIN$allele.frequency[impr_res_FIN$position=="loc202"],
  impr = impr_res_FIN$estimated.i[impr_res_FIN$position=="loc202"],
  SE = SEmedian, inbr = Fmedian, plot_NoImpr = TRUE, SplitPeaks = FALSE) + 
  ggplot2::ggtitle("Control")

RatioCASE <- caseList[["loc202"]]$ref_count / 
  (caseList[["loc202"]]$ref_count + caseList[["loc202"]]$var_count)
CASEDat <- data.frame("Ratio" = RatioCASE)
HistCASE <- ggplot2::ggplot() + ggplot2::geom_histogram(data = CASEDat, ggplot2::aes(Ratio),
  bins = 50) + ggplot2::labs(x="Reference allele fraction", y="Frequency") + 
  ggplot2::ggtitle("Case")

gridExtra::grid.arrange(HistCTRL, HistCASE, ncol=2)

```

# Session info

```{r}
sessionInfo()
```
