---
title: "MAGE tutorial"
author: "Cedric Stroobandt"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette: default
  html_document:
    code_download: yes
    theme: cosmo
    toc: yes
    toc_float: yes
    highlight: tango
    number_sections: yes
vignette: |
  %\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
options(width = 16)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# MAGE: Modeller of Allelic Gene Expression

The MAGE package provides extensive functions for RNAseq-based allelic analyses, ranging from basic tasks such as genotyping - albeit allele-specific-expression (ASE) aware and relying on RNAseq data only - to the analysis of more complex ASE-related phenomena such as allelic bias (AB) detection, differential allelic divergence (AD) detection in case (e.g. tumor) populations compared to controls, and detection of imprinted loci in controls with subsequent detection of loss of imprinting (LOI) in cases. More information on these phenomena is provided throughout this vignette. As MAGE relies on population-level models, large numbers of samples are extremely beneficial (100 or more is ideal; 20 as a bare minimum).

This vignette makes use of *wrapper functions that both enable parallellization and streamline commandos*, those being `AllelicMeta_est_par()`, `BetaBinomGenotyping()`, `dAD_analysis()` and `impr_LOI_analysis()`. Under-the-hood, these call several MAGE functions and perform intermediary processing, which makes for a more readable vignette and an easy-to-use plug-in pipeline, but also make the entire process more of a black box. As such, MAGE also includes an expanded version of this vignette foregoing these wrapper functions, which contains more code but could be interesting for experienced users wanting more insight in, or control over, every step of the pipeline.

# A toy dataset

MAGE's main input consists of per-locus per-sample nucleotide counts (A/T/C/G). These are relatively straightforward to extract from BAM/SAM files via custom scripting or established pipelines such as SAMtools' mpileup or GATK's CollectAllelicCounts functionality (https://gatk.broadinstitute.org/hc/en-us/articles/360037594071-CollectAllelicCounts), though the latter already filters the data down to one reference- and one variant-allele count.

MAGE includes a small example of such data, including both a control (e.g. healthy tissue) and case (e.g. tumor tissue) dataset:

```{r}
data("MAGE", package = "MAGE")
knitr::kable(head(ControlCounts))
```

```{r}
knitr::kable(head(CaseCounts))
```

These toy datasets contain nucleotide counts for 269 loci, every locus being covered by a varying number of samples (e.g. 127 samples covering loc1 in the control dataset):

```{r, results='asis'}
length(unique(ControlCounts$locus_id))
```
```{r, results='asis'}
sum(ControlCounts$locus_id == "loc1")
```

Only a control dataset is required for RNAseq-based genotyping, imprinting detection, and allelic bias detection, but differential analyses studying e.g. the effect of disease on allelic expression (differential allelic divergence, loss of imprinting) require a comparison of cases against controls.

# Input pre-processing

MAGE expects its input data to be formatted as *lists of dataframes, every dataframe containing the per-sample nucleotide counts of one particular locus*, and has some requirements about column naming as well. The first function we'll be using is `standard_alleles()`, which expects the columns `ref_alleles`, `A`, `T`, `C` and `G`. Therefore, here, the current `reference alleles()` column in the toy dataset should be renamed to `ref_alleles` to comply with input requirements; each of these requirements are listed on the respective function's help pages, though besides this initial setup all remaining input requirements will be met if following this entire vignette from start to finish:

```{r}
# MAGE expects lists:
controlList <- list() 
caseList <- list()

for(n in unique(ControlCounts$locus_id)){              # For every locus...
  interDF <- ControlCounts[ControlCounts$locus_id==n,] # extract per-sample nucleotide counts
  colnames(interDF)[2] <- "ref_alleles"                # re-name the reference_allele column 
  controlList[[n]] <- interDF                          # put it into the list
}

for(n in unique(CaseCounts$locus_id)){
  interDF <- CaseCounts[CaseCounts$locus_id==n,]
  colnames(interDF)[2] <- "ref_alleles"
  caseList[[n]] <- interDF
}
```

# Determine one reference and one variant allele

MAGE employs beta-binomial modelling of allele counts, which implies only two alleles are allowed (which is compatible with the large majority of all loci in most diploid species, including humans). Accounting for more alleles would require a different approach, possibilities being the use of multiple beta-binomial models (for all alleles against one set reference, or even all pairwise combinations) or of (beta-)multinomial models”. The latter is, however, not supported by MAGE at the moment.

So as it stands, we need to pick one reference and one variant allele and retain only their count data. This can be done by hand by a researcher based on biological knowledge or through some automated procedure, e.g. retaining only the most common alleles. MAGE's `standard_alleles()` function combines both approaches by taking a look at the standard alleles for every locus according to dbSNP which are supplied by the `ref_alleles` column, then picking the two most common alleles from among these suggested standard alleles only. Should one wish to solely rely on RNAseq count files without external annotation, it's possible to supply `standard_alleles()` with `A/T/C/G` strings in all of its `ref_alleles` columns, which will result in the most abundant alleles being picked.

`standard_alleles()` updates each dataframe in our lists, retaining only two alleles in the `ref_alleles` column and including additional columns specifying which allele is chosen as reference and which as variant allele, alongside their respective counts (the reference will be the most prevalent of the two in terms of total count in all RNAseq data combined for that particular locus):

```{r, warning=FALSE, results = 'asis', message=FALSE}
for(n in names(controlList)){
  controlList[[n]] <- MAGE::standard_alleles(controlList[[n]])
}
knitr::kable(head(controlList[["loc27"]]))
```

We could run `standard_alleles()` on the case data as well, but to enable comparison between both datasets, we enforce picked alleles to be the same for corresponding loci in the control- and case-datasets (this makes sense from a biological perspective as well):

```{r}
for(n in names(caseList)){
  interDF <- caseList[[n]]
  interDF$ref_alleles <- controlList[[n]]$ref_alleles[1]
  interDF$ref <- controlList[[n]]$ref[1]
  interDF$var <- controlList[[n]]$var[1]
  interDF$ref_count <- interDF[,which(colnames(interDF)==interDF$ref[1])]
  interDF$var_count <- interDF[,which(colnames(interDF)==interDF$var[1])]
  
  caseList[[n]] <- interDF
}
```

# Prior filtering

Before doing any analyses, it might be beneficial (in terms of runtime) to filter out low-quality loci. MAGE's `prior_filter()` offers this functionality, leaving out loci based on a minimal median coverage across all samples, a minimal number of samples, or a crudely estimated minimal minor allele frequency (just the % abundance of the variant allele over all RNAseq counts). After all, loci with a too low minor allele frequency will feature insufficient numbers of heterozygous individuals, which are required to observe ASE effects. Also, if the `checkref_filter` argument is set to TRUE, `prior_filter()` checks whether the total nucleotide counts truly suggest there's only two occurring alleles as assumed in the previous step, or whether there's heuristic evidence to the contrary, in which case the locus is removed (details are found on this function's help page).

For now, we'll restrict ourselves to just a minimum number of samples filter of 20 in controls (only counting samples with at least one reference- or variant-read). The first analysis is genotyping after all, and researchers might be interested in getting the most likely genotype for every sample regardless of median coverage or minor allele frequency across the entire locus (as MAGE is a population-level modeller a minimal number of samples of at least 20 is always advised). We apply no actual filtering on the case dataset however, but rather retain a locus' data if it's retained in the control set as well, as comparison against controls is the case dataset's main purpose. We do run `prior_filter` on cases with no actual filter settings applied though, which reduces the function to simply removing samples having a zero-count for both reference- and variant-alleles (not removing these would give NA-results during several calculations in the pipeline). This reduces the toy datasets from 269 to 264 loci:

```{r}
for(n in names(controlList)){
  controlList[[n]] <- MAGE::prior_filter(controlList[[n]], min_median_cov = 0, 
    min_nr_samples = 20, checkref_filter = TRUE, prior_allelefreq_filter = FALSE, 
    min_PriorAlleleFreq = 0)
  caseList[[n]] <- MAGE::prior_filter(caseList[[n]], min_median_cov = 0, 
    min_nr_samples = 0, checkref_filter = FALSE, prior_allelefreq_filter = FALSE, 
    min_PriorAlleleFreq = 0)
  if(is.null(controlList[[n]])){
    caseList[[n]] <- NULL
  }
}
```

# Enabling parallellization

Before any further analyses, this vignette first splits input data lists into `NC` chunks below as a nested list, `NC` being a number of cores as the coding in this vignette allows MAGE to be run in parallel on a server using R's `parallel` package. A local (Windows) installation is only able to use 1 core at a time though, so `NC` is set to one in the code below. Feel free to change it when running MAGE on e.g. a linux-based server:

```{r}
NC <- 1 # Number of Cores
NS <- length(controlList) 
spl <- c(0, cumsum(rep(floor(NS/NC),NC)+c(rep(1,NS-floor(NS/NC)*NC),
         rep(0,NC-NS+floor(NS/NC)*NC)))) # Helps in splitting input data
ParCTRL <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParCTRL[[i]] <- controlList[(spl[i]+1):(spl[i+1])]
}
ParCASE <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParCASE[[i]] <- caseList[(spl[i]+1):(spl[i+1])]
}
```

# Metaparameter estimation

Before model fitting, we need to estimate the population-level metaparameters shared across all loci, those being the inbreeding coefficient of the population and the sequencing error rate. These values can be set based on prior knowledge about the population and the sequencing protocol of choice, but MAGE's `AllelicMeta_est()` can estimate them based on observed *control* data, which is recommended anyway for a good model fit. This function fits a regular binomial mixture model assuming no allelic bias (i.e. perfectly balanced expression of the reference- and variant allele in heterozygotes) to control read count data via expectation maximization:

$$
\small
\begin{aligned} 
\mathrm{PMF}(counts_{ref}|counts_{total}) = &\ P_{rr} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=1-SE) \ + \\
&\ P_{rv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=0.5) \ + \\
&\ P_{vv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=SE) \\
\end{aligned}
$$

With $(P_{rr}, P_{rv}, P_{vv})$ to-be-fitted genotype frequencies from which the inbreeding coefficient can be estimated, and SE a to-be-fitted sequencing error rate estimate. The function also returns a number of outputs that can be used as reliability measures, including a preliminary reference allele frequency estimate across genotypes (from which the minor allele frequency can be calculated), median coverage and number of samples. The SE-estimate itself shouldn't be absurdly high either. Default cutoff values for these quality metrics are suggested in the code below (`pA_filt` dictating minimal estimated minor allele frequency). We *don't* filter these loci out however; a suboptimal fit by `AllelicMeta_est()` may be remedied by the later beta-binomial fit allowing unequal heterozygous allele expression for actual genotyping and allelic bias detection.

Note that the function being called is `AllelicMeta_est_par()`, a wrapper function of `AllelicMeta_est()` handling all steps described above and providing, besides updated control-count-dataframes, vectors containing reliable metaparameter estimates (of which one still has to take e.g. the mean or median to get final metaparameters estimates; in this vignette, we use the more robust median). As mentioned, additional insight into this wrapper function can be learned from its source code, or in MAGE's extended vignette:

```{r, results='asis'}
cl <- parallel::makeCluster(getOption("cl.cores", NC))
GenoFinData <- parallel::parLapply(cl, X = ParCTRL, fun = MAGE::AllelicMeta_est_par,
  pA_filt = 0.15, SE_filt = 0.035, NumSamp_filt = 20, MedianCov_filt = 4)
parallel::stopCluster(cl)

ParCTRL <- lapply(GenoFinData, `[[`, 1)
controlList <- do.call(c, lapply(GenoFinData, `[[`, 1))
SE_vec <- do.call(c, lapply(GenoFinData, `[[`, 2))
F_vec <- do.call(c, lapply(GenoFinData, `[[`, 3))

SEmedian <- median(SE_vec)
Fmedian <- median(F_vec)

#estimates of resp. sequencing error rate and inbreeding coefficient:
print(c(SEmedian, Fmedian)) 
```

These metaparameter estimates seem reasonable. Please note that an extremely low sequencing error metaparameter would make fitting of homozygous peaks later down the pipeline way too strict, failing to accomodate for even a single faulty read. As such, *setting SE lower than 0.002 is not recommended*. An inbreeding coefficient close to zero is a sign of a panmictic population and commonplace when working on human data, but it could be more extreme in specialized experimental setups.

# Genotyping RNAseq data & allelic bias detection

For both genotyping and allelic bias detection, we can use MAGE's `EMfit_betabinom_robust()` function, which fits a beta-binomial mixture model using expectation maximization while allowing for unequal allelic expression in heterozygotes:

$$
\small
\begin{aligned} 
\mathrm{PMF}(counts_{ref}|counts_{total}) = &\ P_{rr} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=1-SE, \theta=\theta_{hom}) \ + \\
&\ P_{rv} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=\pi_{het}, \theta=\theta_{het}) \ + \\
&\ P_{vv} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=SE, \theta=\theta_{hom}) \\
\end{aligned}
$$

With $(P_{rr}, P_{rv}, P_{vv})$ fitted genotype frequencies, $(\theta_{hom}, \theta_{het})$ fitted overdispersion parameters in homozygotes and heterozygotes respectively, and $\pi_{het}$ the heterozygous $\pi$ parameter indicating the expected reference allele fraction, thus allowing for unequal allelic expression in heterozygotes. `pBetaBinom()` is a function included in the MAGE package for the beta-binomial PMF, using a $\pi, \theta$ parameterisation (probability of success, here defined as observing a reference read, and overdispersion, respectively) instead of the classical $\alpha, \beta$ parameterisation as given on e.g. https://en.wikipedia.org/wiki/Beta-binomial_distribution. The relationship between them is $\pi = \alpha/(\alpha+\beta)$ and  $\theta = 1/(\alpha+\beta)$, resulting in a $\pi$ between 0 and 1 and a $\theta$ between 0 and infinity.

We term unequal heterozygous allele expression captured by $\pi_{het}$ *Allelic Bias (AB)*. AB can be caused by both biological (e.g. cis-eqtl loci) and technical (e.g. alignment bias) mechanisms. The fit is robust in the sense that samples with a disproportionately high impact on the heterozygous distributional parameters (determined using parameter-MLEs leaving out that particular sample) are not used for the final fit, though they are retained in the output data and genotyped using the final model (while being marked as outlier). The reason for focussing on heterozygous distributional parameters is that these are still being fitted (the homozygous $\pi$ parameters are already fixed by the metaparameter SE) and are actually affected by allele-specific expression effects ($\pi_{het}$ can indicate AB and $\theta{het}$ can indicate allelic divergence, see later; meanwhile $\theta_hom$ is a nuisance parameter at best). Do note, however, that the robust fit is significantly slower than the non-robust one provided by the `EMfit_betabinom()` function (proportional to the number of samples), and its merit for genotyping and AB-detection is limited, so one might opt for the latter instead under certain circumstances (refer to MAGE's extended vignette and modify the pipeline accordingly if you wish to de this). A robust fit is, however, very valuable for statistical inference on the overdispersion parameter $\theta$ (see the next section).

MAGE's functions have many optional input parameters to customize the fitting procedure, but throughout this tutorial and for most applications the defaults should suffice. Feel free to read up on all options on each function's help page. After performing the fit and writing results of interest into dataframes, we also perform a chi-square test assessing Hardy-Weinberg Equilibrium using our final genotypes and the previously determined inbreeding coefficient hyperparameter, to be used for filtering later.

All steps described above are handled by `BetaBinomGenotyping()`, a wrapper function of `EMfit_betabinom_robust()`, `median_AB()`, and `HWE_chisquared()`, which returns updated per-locus dataframes with information of the separate samples (e.g. genotype probabilities) as well as a dataframe containing AB-analysis results (see `Geno_AB_res` below). Please refer to `BetaBinomGenotyping()`'s source code or MAGE's extended vignette for additional insight into all component steps.

```{r, warning=FALSE, message=FALSE}
cl <- parallel::makeCluster(getOption("cl.cores", NC))
GenoFinData <- parallel::parLapply(cl, X = ParCTRL, fun = MAGE::BetaBinomGenotyping, 
                                   SE = SEmedian, inbr = Fmedian)
parallel::stopCluster(cl)

ParCTRL <- lapply(GenoFinData, `[[`, 1)
controlList <- do.call(c, lapply(GenoFinData, `[[`, 1))
Geno_AB_res <- do.call("rbind", lapply(GenoFinData, `[[`, 2))
```

As a sidenote, while this vignette represses warnings, calling upon MAGE's beta-binomial density function (`pBetaBinom()`, which `EMfit_betabinom_robust()` does) can result in a memory limit warning which can usually be ignored; most of the time it signifies that a certain locus' beta-binomial fit was so close to a regular binomial fit that the overdispersion parameter is extremely close to zero, leading to precision errors in beta-binomial calculations which will trigger MAGE to use regular binomial functions instead.

The genotyping results can now be retrieved from the `controlList` object, being included as the "genotypeN" column in each locus' dataframe:

```{r}
knitr::kable(head(controlList[["loc27"]][,c("locus_id", "sample_id", "ref", 
                                            "var", "genotypeN")]))
```

The `Geno_AB_res` dataframe contains the results of AB-detection, i.e. testing whether the reference allele expression ratio differs from a "perfectly balanced" ratio of 0.5, i.e. is there a preference to detect one of both alleles? The "probshift" column contains the actual fitted allelic ratio while the "p" column contains the p-value of a likelihood-ratio-test assessing significant deviance from 0.5; both can be filtered on to retain only significantly allelically biased loci with a big enough effect size. 

Other columns contain additional filter criteria, like

* "median_AB": a median (robust) reference allele ratio estimate in case you want to play it extra safe in retaining significantly biased loci, see `median_AB()`'s help page for more information
* "quality": will contain "!" if a locus contained no heterozygous samples to detect AB on
* "allele.frequency": referring to the reference allele frequency across genotypes; you may not want to trust loci with extreme allele frequencies as these probably contain very little heterozygous individuals
* "coverage": contains the median coverage of all samples covering the locus)
* "nr_samples": number of samples covering the locus

The code below first retains loci showcasing statistically significant AB (controlling the false discovery rate at 0.05) which are of good quality. Then, we also construct a more high-fidelity set of loci, retaining only those of good quality with a fitted allelic ratio greater than 0.6 or smaller than 0.4, yet not more extreme than 0.9 or 0.1 because those could be the product of poorly fitted loci. The median ratio should also be larger than 0.6 or smaller than 0.4, allele frequency across genotypes should not be more extreme then 0.9 or 0.1, median coverage should be over 10 and number of samples should be over 80. These are reasonable filter criteria, though their specific values depend on each end user's desires and the specific dataset used (e.g. requiring over 80 samples is perhaps too stringent for smaller datasets and too liberal for larger ones).

In BOTH CASES, we *only retain loci that are in Hardy-Weinberg Equilibrium (HWE)* according to the chi square test performed earlier. Since we retain only those loci for which we can't find sufficient evidence against the null hypothesis of HWE, this is a statistically weak conclusion, yet in scientific research a cut-off of the p-value of this test of 0.001 is often used to filter out loci with which something seems wrong (see e.g. \insertCite{sha}{MAGE}, \insertCite{teo}{MAGE}, \insertCite{rohlfs}{MAGE}); such extreme deviations from HWE should not occur in a normal population.

```{r, results='asis'}
# Statistical evidence for significant AB at the 5% FDR level:
print(paste(Geno_AB_res$position[p.adjust(Geno_AB_res$p, method = "BH") < 0.05 & 
      Geno_AB_res$quality != "!" & Geno_AB_res$Chi2PVAL >= 0.001], collapse = ", "))

```

```{r, results='asis'}
# Statistical evidence for significant AB at the 5% FDR level,
# only retaining reliable (high-quality) loci with a large enough effect size:
print(paste(Geno_AB_res$position[p.adjust(Geno_AB_res$p, method = "BH") < 0.05 & 
  Geno_AB_res$quality != "!" & (Geno_AB_res$probshift > 0.6 | Geno_AB_res$probshift < 0.4) 
  & Geno_AB_res$probshift < 0.9 &  Geno_AB_res$probshift > 0.1 & (Geno_AB_res$median_AB > 0.6 | 
  Geno_AB_res$median_AB < 0.4) & Geno_AB_res$allele.frequency < 0.9 & 
  Geno_AB_res$allele.frequency > 0.2 & Geno_AB_res$coverage > 10 & 
  Geno_AB_res$nr_samples > 80 & Geno_AB_res$Chi2PVAL >= 0.001], collapse = ", "))

```

MAGE contains a function to plot histograms of the reference allele fraction across samples for a specific locus, while also including PMF plots of both the unshifted (assuming no AB) and shifted models. The theta-parameters for the unshifted model are included in the previous `EMfit_betabinom_robust()` output as they are calculated anyway to test for a significant shift using a likelihood ratio test. This plot can illustrate just how much better the shifted model fits to the data, as illustrated below. 

Keep in mind, however, that for a beta-binomial PMF, its shape (variance) depends on the coverage is inferred from, even when normalized from the reference allele count to a 0-to-1 reference allele fraction, such as on the x-axis in the plot below. As such, we have to pick a coverage to use as basis for the PMF plots (`ScaleCount` argument). The median coverage across samples is a good pick for this, though for data points with outlying coverages, the resulting visualization may still be poor (even if those data points are well fitted by the model). This is because the fitted model allows for a flexible per-sample coverage parameter, whereas for our PMF plot, we have to pick just one.

To accomodate for this, one might opt to set the `ScaleHist` argument to TRUE, which will re-calculate observed per-sample reference allele fraction that are to be plotted in the histogram using the MAGE's beta-binomial quantile function given the observed per-sample data probability as input, but while using `ScaleCount` total counts (i.e. we transform observed reference allele fractions to the fractions we "would have observed" - i.e. with the same cumulative distribution function value - given `ScaleCount` observed reads, assuming the fitted beta-binomial model is correct). It's up to the end user to use this argument, though for a comfortable interpretation of the plots it's recommended if the per-sample coverage is very variable.

`MAGE_EMfitplot()` directly accepts the output of the `BetaBinomGenotyping()` wrapper function to fetch distributional parameters from, which is handy when following this vignette. All of these parameters can also be inputted manually, however, which is illustrated in MAGE's extended vignette.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
PlotDF <- controlList[["loc11"]]
PlotAB <- Geno_AB_res[Geno_AB_res$position=="loc11",]

loc_plot <- MAGE::MAGE_EMfitplot(DataList_out = PlotDF, Geno_AB_res = PlotAB, SE=SEmedian, 
  ScaleCount = PlotAB$coverage, ScaleHist = TRUE, plot_NoShift = TRUE, nbins = 30)

loc_plot
```

The plotting function contains some customization options, the plot above mainly using default colors with the PMF of the shifted fit split up according to genotype. One can e.g. choose to plot one combined unshifted PMF, and to not plot the unshifted fit, via the code below (once again using default colors, though these can all be changed):

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
loc_plot <- MAGE::MAGE_EMfitplot(DataList_out = PlotDF, Geno_AB_res = PlotAB, SE=SEmedian, 
  ScaleCount = PlotAB$coverage, ScaleHist = TRUE, plot_NoShift = FALSE,
  SplitPeaks = FALSE, nbins = 30)

loc_plot
```

# Differential allelic divergence detection

In the context of MAGE, we see case-specific events such copy number alterations, aberrant hypermethylation, promotor mutation and silencing events of tumor suppressor genes as mechanisms that will affect one allele, or both alleles in a different way, per sample. However, these mechanisms show no preference towards either allele on a population-level, acting mainly as an added source of variance on the observed reference allele fraction captured by $\theta_{het}$ while leaving $\pi_{het}$ unchanged. Calling this variance *Allelic Divergence (AD)*, we term its change (increase) in disease due to, amongst others, the mechanisms listed above *differential Allelic Divergence (dAD)*.

Detecting dAD between to populations can be done via likelihood ratio test, comparing: (1) a model fitting ALL data, both control and case, as one beta-binomial mixture model, with (2) a model allowing the control- and tumor data to have their separate $\theta$ parameters for the heterozygous samples, yet keeping the $\pi$ parameter constant. The pivotal new function is `EMfit_betabinom_popcomb()` which fits the model assuming different heterozygote-thetas while keeping all other parameters shared. It is itself included in the `dAD_analysis()` wrapper function, which also included `EMfit_betabinom_robust()`, `EMfit_betabinom()`, `pmf_betabinomMix()`, and `HWE_chisquared()`. The dAD-detecting pipeline is relatively complex in the way the various fits and outlier detection are handled, so checking out `dAD_analysis()`'s source code or the extended vignette is certainly recommended for additional insight. Here, however, `dAD_analysis()` conveniently returns a dataframe containing per-locus (d)AD results: `dAD_res`.

`dAD_analysis()` requires control- and case-input lists to be combined into one (`ParTOT` below), which is only done now so as to contain the updated `ParCTRL` list as outputted by `BetaBinomGenotyping()`.

```{r warning=FALSE}
# Only make ParTOT now so it contains the updated ParCTRL
ParTOT <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParTOT[[i]] <- list(ParCTRL[[i]], ParCASE[[i]])
}

cl <- parallel::makeCluster(getOption("cl.cores", NC))
dADFinData <- parallel::parLapply(cl, X = ParTOT, fun = MAGE::dAD_analysis, SE=SEmedian, inbr = Fmedian)
parallel::stopCluster(cl)

dAD_res <- do.call("rbind", dADFinData)
```

In `dAD_res`, both `theta`- and `rho`-values are listed, which both represent the overdispersion parameter; $\theta$ just has a range from 0 to infinity while $\rho$ has been rescaled to go from 0 to 1, which is more convenient for some applications, such as plotting. When studying differential allelic divergence, one usually tries to look for "disturbed regions" by plotting the results in `dAD_res` among the length of chromosomes (e.g. plotting case rho-values along the chromosomes while also marking the actually statistically relevant loci in some way), as will be demonstrated at the end of this section.

Note that both parameters are perfectly equivalent, and that the exact parametrization does not impact the statistical testing procedure (as both will yield exactly the same likelihoods).

That being said, you can also just extract loci showing differential allelic divergence from `dAD_res` using the `LRTpval` column to retain statistically significant results. Defining an effect size is a bit trickier; you'd expect to be able to do it on the overdispersion parameter values of cases compared to controls, but how would one go about it? When both are very small (e.g. rho ~ 10^-7 range), one being 10 times as large as the other doesn't mean that much, but when both are quite large (e.g. rho ~ 10^-2 range) this does make a very large difference. Subtracting one from the other has an equally unclear interpretation regarding effect size. The effect of the overdispersion parameter on the actual variance also depends on the pi parameter, which further complicates things. Moreover, in many cases, e.g. cancer, case samples will be mixtures of normal and diseased cells, with varying (gene-dependent) levels of expression in each cell type, further complicating the creation of appropriate effect sizes.

This type of difficulties made us opt for detecting affected regions on plots rather than relying on the effect sizes as such. But if you wanted to e.g. only retain loci that show a strong deviation from a regular binomial regarding its variance, using a rho-cutoff of, say, 0.1, can do. Such cutoffs are based on experience and can be played around with. In the code below, we retain such good-quality loci which are also statistically significant regarding dAD and have a median coverage of at least 10 in both controls and cases, as well as at least 15 heterozygous samples and data conform Hardy-Weinberg Equilibrium in controls only (reliable case data genotyping may be difficult when faced with extreme differential allelic divergence). Also, note that for genuine differential allelic divergence, overdispersion will be typically be larger in cases than in controls.

```{r, results='asis'}
print(paste(dAD_res$LocName[dAD_res$RhoHetCASE >= 0.1 & p.adjust(dAD_res$LRTpval, 
  method = "BH") < 0.05 & dAD_res$CovCTRL_med >= 10 & dAD_res$CovCASE_med >= 10 & 
  dAD_res$NumHetCTRL >= 15 & dAD_res$HWECTRL >= 0.001 & 
  dAD_res$RhoHetCASE > dAD_res$RhoHetCTRL], collapse = ", "))
```

The previously used `MAGE_EMfitplot()` function can also be used to vizualize the dAD detection results via separate plots of the control- and case-population utilizing their separately fitted overdispersion parameters. Here, it's important to set the input argument `ScaleCount` to the same value for both plots, so the width of the PMFs can actually be compared correctly. When following this vignette, it once again accepts the (d)AD-results dataframe of `dAD_analysis()` as an input to fetch distributional parameters from (manual entry of these parameters is demonstrated in the extended vignette). Be aware setting the `plot_NoShift` argument to TRUE is not an option here, as the dAD-analysis results do not include AB-related results such as distributional parameters of the unshifted fit, and the extra `PlotWhich` argument ("control" or "case") is required for the function to know which parameters to fetch from `dAD_res`.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
CTRL_DF <- controlList[["loc85"]]
CASE_DF <- caseList[["loc85"]]
PlotData_dAD <- dAD_res[dAD_res$LocName=="loc85",]

dAD_plot1 <- MAGE::MAGE_EMfitplot(DataList_out = CTRL_DF, dAD_res = PlotData_dAD, 
  SE = SEmedian, ScaleCount = 50, ScaleHist = TRUE, nbins = 30, plot_NoShift = FALSE,
  PlotWhich = "control")

dAD_plot2 <- MAGE::MAGE_EMfitplot(DataList_out = CTRL_DF, dAD_res = PlotData_dAD, 
  SE = SEmedian, ScaleCount = 50, ScaleHist = TRUE, nbins = 30, plot_NoShift = FALSE,
  PlotWhich = "case")

gridExtra::grid.arrange(dAD_plot1, dAD_plot2, ncol=2)
```

The plot above is of a locus showcasing very clear differential allelic divergence, with the reference allele fraction in heterozygous case samples being way more variable, probably due to case-specific effects.

As promised, to conclude, this section showcases a plot of dAD results across a chromosome using the `MAGE_ADChromplot()` function. The toy data isn't fit for this, however (no chromosomal location, too small), so included in the MAGE package is some thoroughly processed data to illustrate the plot on. The inputs needed are $\rho$ values in both controls and cases along with differential expression data (log2 fold changes) which can be easily generated by tools for differential expression analysis on the data at hand. `MAGE_ADChromplot()` optionally even accepts additional hypermethylation and copy-number-alteration data for even more extensive plots. As this is for illustrative purposes only, we just make the plot using the provided datasets below, but more details can be learned from `MAGE_ADChromplot()`'s help page. The function returns several plot components (including legends) as separate objects due to its complexity, so it's up to the end user to arrange them into a satisfactory composition.

As a final remark, many of the measures in this plot (differential expression, hypermethylation, copy number alterations) are usually obtained at the gene-level rather than the SNP/locus level. To make the results of different analyses more compareable, MAGE's per-locus p-values can be combined into per-gene values using the `combine_p_gene()` function. For this, we recommend using the geometric mean as a nice balance between more conservative (arithmetic mean, maximum) and liberal (harmonic mean, minimum) methods for combining p-values, and the different loci should be weighted as well, for which we recommend $\small \sqrt{median \ locus \ coverage \ * \ estimated \ number \ of \ heterozygous \ samples }$, the latter being an output of MAGE's beta-binomial mixture model fits via the heterozygote genotype frequency times the number of samples. For p-values concerning a comparison between a control- and case-population, one can e.g. use the minimum of this weight across populations.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
ChromPlot <- MAGE::MAGE_ADChromplot(AD_Data, DE_Data, Meth_Data, CNAgain_Data, CNAloss_Data,
                                    pvalSIG = 0.05, roll_median = 15)

ChromPlot[["ADDE_plot"]] / ChromPlot[["MethCNA_plot"]] / (ChromPlot[["LEG1"]] + 
  ChromPlot[["LEG2"]] + ChromPlot[["LEG3"]]) + 
  patchwork::plot_layout(heights = c(2,1,0.5), widths = c(1,1,1))
```

The histograms in the top panes show (rolling medians) of the overdisperstion ($\rho$) parameters in controls (black) and cases (colored). The case-$\rho$ is colored according to the (already 5% FDR corrected) p-value testing for significant dAD in cases versus controls, while the relative heights of these histograms provide an illustration of the effect size. The remaining plot elements are explained in the legend.

# Detecting imprinted and differentially imprinted loci

Imprinting analyses in MAGE are quite different from the analyses up until now in that they don't rely on beta-binomial modelling and make different assumptions about the data. They are, generally speaking, a bit more robust because imprinting is usually an extreme phenomenon, i.e. heterozygous samples of imprinted loci will almost always "seem like" homozygous samples because one of the alleles has been silenced almost entirely. Partial imprinting can exist and MAGE can detect it, but this phenomenon is not modelled as thoroughly as the previous ones in this vignette.

First off (besides setting a minimal required minor allele frequence of 0.15 as detecting imprinting can otherwise be unreliable) we assume samples show no AB (as we'll be using unshifted models) and are conform Hardy-Weinberg-Equilibrium while taking imprinting into account, the latter of which is tested by `symmetry_gof()`. It does this by assessing whether the number of samples with a reference allele fraction > 0.5 (and so also the number with the fraction <= 0.5) is equal to that expected under HWE assuming no AB using a chi square test. This may seem kind of heuristic at first and prone to fail if there is even a little AB, but remember we want to mainly retain imprinted loci, meaning heterozygous samples should behave as one of both homozygotes (and so clearly feature reference allele fractions > 0.5 or <=0.5) so this filter is very fitting for retaining HWE-conform samples that are at the same time likely to showcase imprinting. A good filter setting is requiring the p-value to be at least 0.05.

After applying these filters, `imprinting_est()` detects imprinted loci in the remaining control samples by first assuming an unshifted binomial mixture model, then splitting up the heterozygous samples in two separate groups according to a degree of imprinting $i$ and varying this $i$ from 0 to 1 in a for-loop, calculating likelihoods along the way to perform a likelihood-ratio test to check for differential imprinting (i.e. compare the most likely model with the non-imprinted one; $i=0$). For completeness, the imprinted PMF is (see \insertCite{goovaerts2018}{MAGE} for a full elaboration):

$$
\small
\begin{aligned} 
\mathrm{PMF}(&counts_{ref}|counts_{total}) =  P_{rr} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=1-SE) \ + \\
& 0.5P_{rv} * {\tt pbinom} \left( x=counts_{ref} \ |\ n=counts_{total}, p=\frac{0.5-\frac{i}{2}}{1-\frac{i}{2}}(1-SE)+\frac{0.5}{1-\frac{i}{2}}SE\right) \ + \\
& 0.5P_{rv} * {\tt pbinom} \left( x=counts_{ref} \ |\ n=counts_{total}, p=\frac{0.5-\frac{i}{2}}{1-\frac{i}{2}}SE+\frac{0.5}{1-\frac{i}{2}}(1-SE)\right) \ + \\
& P_{vv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=SE)
\end{aligned}
$$
Note that this imprinting model consisting of regular binomial functions also lead us to using preliminary minor allele frequency estimates as produced by `AllelicMeta_est_par()` towards the start of the vignette for the aforementioned 0.15 minor allele frequency filtering, as it assumes a similar (non-beta-binomial) mixture model. This becomes clear in MAGE's extended vignette, but is here hidden under the `impr_LOI_analysis()` wrapper.

After retaining only significantly and sufficiently imprinted loci (5% FDR level, degree of imprinting at least 0.6 and median degree of imprinting at least 0.8 for robustness; see `median_imprinting()` for the latter's calculation) using the `final_filter()` function, we use `LOItest_logreg()` to test for differential imprinting. More specifically, logistic regression is used to evaluate whether case samples feature a significantly higher number of apparently heterozygous samples than control samples (by considering the least expressed allele as a success; more details can be found on the function's help page; note that the logistic regression approach takes into account variable coverage between samples). If this is true, it indicates that a significant fraction of heterozygous samples in tumor tissue "lost their imprinting" and actually started expressing both alleles again. We call these loci “differentially imprinted”, as we reserve the term “loss-of-imprinting” for the phenomenon where differential imprinting co-occurs with increased total expression, indicative of re-expression of the silenced allele.

All of these steps are combined into the `impr_LOI_analysis()` wrapper function, calling `symmetry_gof()`, `imprinting_est()`, `median_imprinting()`, `final_filter()`, and `LOItest_logreg()`. Extra insight can be learned from `impr_LOI_analysis()`'s source code or MAGE's extended vignette, which includes a lot of clarifying comments. In the end, `impr_LOI_analysis()` returns imprinting-detection results per locus (`impr_res`) and, for those loci that are significantly and sufficiently imprinted, LOI-detection results as well (`LOI_res`)

```{r warning=FALSE}
cl <- parallel::makeCluster(getOption("cl.cores", NC))
ImprFinData <- parallel::parLapply(cl, X = ParTOT, fun = MAGE::impr_LOI_analysis, SE = SEmedian, inbr = Fmedian,
  MinMinorAllelefreq=0.15, sym_filt=0.05, adj_p_filt=0.05, gof_filt=0.8, med_i_filt=0.8, i_filt=0.6)
parallel::stopCluster(cl)

impr_res <- do.call("rbind", lapply(ImprFinData, `[[`, 1))
LOI_res <- do.call("rbind", lapply(ImprFinData, `[[`, 2))
```

We can take a look at the loci that are significantly and sufficiently imprinted (i.e. those retained in `LOI_res`):

```{r, results='asis'}
knitr::kable(head(LOI_res))
```

```{r, results='asis'}
print(paste(LOI_res$position, collapse = ", "))
```

From among this set, a part is actually differentially imprinted in case samples, which is indicated by the p-value in the `DI_pval` column in the `LOI_res` dataframe (the `adj_p` p-value detects imprinting in controls, FDR-corrected; these should all be significant in `LOI_res`, though not necessarily in `impr_res`). We can control differential imprinting detection, for a change, at the 5% FWER level (actually feasible here due to the small number of actually imprinted loci remaining, though we're looking at a small toy dataset here; this isn't necessarily the case in larger experiments):

```{r, results='asis'}
print(paste(LOI_res$position[p.adjust(LOI_res$DI_pval, method = "holm") < 0.05], 
            collapse = ", "))
```

We can plot histograms of both the control and case allele fractions; locus 202 is obviously imprinted in controls and loses a lot of this imprinting in cases (controls are plotted using `MAGE_imprintplot()` to visualize imprinting results, but since an imprinting-estimation never happens on the case data - we just detect a change in heterozygosity via logistic regression during LOI-detection - we simply make a histogram for cases instead).

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
HistCTRL <- MAGE::MAGE_imprintplot(controlList[["loc202"]]$ref_count, 
  controlList[["loc202"]]$var_count, 
  allelefreq = impr_res$allele.frequency[impr_res$position=="loc202"],
  impr = impr_res$estimated.i[impr_res$position=="loc202"],
  SE = SEmedian, inbr = Fmedian, plot_NoImpr = TRUE, SplitPeaks = FALSE) + 
  ggplot2::ggtitle("Control")

RatioCASE <- caseList[["loc202"]]$ref_count / 
  (caseList[["loc202"]]$ref_count + caseList[["loc202"]]$var_count)
CASEDat <- data.frame("Ratio" = RatioCASE)
HistCASE <- ggplot2::ggplot() + ggplot2::geom_histogram(data = CASEDat, ggplot2::aes(Ratio),
  bins = 50) + ggplot2::labs(x="Reference allele fraction", y="Frequency") + 
  ggplot2::ggtitle("Case")

gridExtra::grid.arrange(HistCTRL, HistCASE, ncol=2)
```

# Session info

```{r}
sessionInfo()
```
