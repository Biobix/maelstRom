---
title: "MAGE tutorial"
author: "Cedric Stroobandt"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette: default
  html_document:
    code_download: yes
    theme: cosmo
    toc: yes
    toc_float: yes
    highlight: tango
    number_sections: yes
vignette: |
  %\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
options(width = 16)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# MAGE: Modeller of Allelic Gene Expression

The MAGE package provides extensive functions for RNAseq-based allelic analyses, ranging from basic tasks such as genotyping - albeit allele-specific-expression (ASE) aware and on RNAseq data only - to the analysis of more complex ASE-related phenomena such as allelic bias (AB) detection, differential allelic divergence (AD) detection in case (tumor) populations compared to controls, and detection of imprinted loci with subsequent detection of loss of this imprinting (LOI) in cases. More information on these phenomena is provided throughout this vignette. As MAGE relies on population-level models, large numbers of samples are extremely beneficial.

# A toy dataset

MAGE's main input consists of per-locus per-sample nucleotide counts (A/T/C/G). These are relatively straightforward to extract from BAM/SAM files via custom scripting or established pipelines such as GATK's CollectAllelicCounts functionality (https://gatk.broadinstitute.org/hc/en-us/articles/360037594071-CollectAllelicCounts), though the latter already filters the data down to one reference- and one variant-allele count.

MAGE includes a small example of such data, including both a control (e.g. healthy tissue) and case (e.g. tumor tissue) dataset:

```{r}
data("MAGE", package = "MAGE")
knitr::kable(head(ControlCounts))
```

```{r}
knitr::kable(head(CaseCounts))
```

These toy datasets contain nucleotide counts for 269 loci, every locus being covered by a varying number of samples (e.g. 127 samples covering loc1 in the control dataset):

```{r, results='asis'}
length(unique(ControlCounts$locus_id))
```
```{r, results='asis'}
sum(ControlCounts$locus_id == "loc1")
```

Only a control dataset is required for RNAseq-based genotyping and allelic bias detection, but differential analyses studying e.g. the effect of disease on allelic expression (differential allelic divergence, loss of imprinting) require a comparison of cases against controls.

# Input pre-processing

MAGE expects its input data to be formatted as *lists of dataframes, every dataframe containing the per-sample nucleotide counts of one particular locus*, and has some requirements about naming as well. The first function we'll be using is `standard_alleles()` which expects the columns `ref_alleles`, `A`, `T`, `C` and `G`. As such, the `reference alleles` column in the toy dataset should be renamed to `ref_alleles`; each function's input requirements are listed on their respective help pages.

```{r}
# MAGE expects lists:
controlList <- list() 
caseList <- list()

for(n in unique(ControlCounts$locus_id)){              # For every locus...
  interDF <- ControlCounts[ControlCounts$locus_id==n,] # extract per-sample nucleotide counts
  colnames(interDF)[2] <- "ref_alleles"                # re-name the reference_allele column 
  controlList[[n]] <- interDF                          # put it into the list
}

for(n in unique(CaseCounts$locus_id)){
  interDF <- CaseCounts[CaseCounts$locus_id==n,]
  colnames(interDF)[2] <- "ref_alleles"
  caseList[[n]] <- interDF
}
```

# Determine one reference- and one variant allele

MAGE employs beta-binomial modelling of allele counts, which implies only two alleles are possible (which is often the case in human populations anyway). Accounting for more alleles would require a different approach, possibilities being multiple beta-binomial models for all alleles against one set reference or even all pairwise combinations, or (beta-)multinomial models; the latter is, however, not supported by MAGE at the moment.

So as it stands, we need to pick one reference and one variant allele and retain only their count data. This can be done by hand by a researcher based on biological knowledge or through some automated procedure, e.g. retaining only the most common alleles. MAGE's `standard_alleles()` function combines both approaches by taking a look at the standard alleles for every locus according to dbSNP which are supplied by the `ref_alleles` column, then picking the two most common alleles from among these suggested standard alleles only. Should one wish to solely rely on RNAseq count files without external annotation, it's possible to supply `standard_alleles()` with `A/T/C/G` strings in all of its `ref_alleles` columns, which will result in the most abundant alleles being picked.

`standard_alleles()` updates each dataframe in our lists, retaining only two alleles in the `ref_alleles` column and including additional columns specifying which allele is chosen as reference and which as variant allele, alongside their respective counts (the reference will be the most prevalent of the two in terms of total count in all RNAseq data combined):

```{r, warning=FALSE, results = 'asis', message=FALSE}
for(n in names(controlList)){
  controlList[[n]] <- MAGE::standard_alleles(controlList[[n]])
}
knitr::kable(head(controlList[["loc27"]]))
```

We could run `standard_alleles()` on the case data as well, but to make comparisons between the two datasets possible we enforce these to be the same for corresponding loci in the control- and case-datasets (this makes sense from a biological perspective as well):

```{r}
for(n in names(caseList)){
  interDF <- caseList[[n]]
  interDF$ref_alleles <- controlList[[n]]$ref_alleles[1]
  interDF$ref <- controlList[[n]]$ref[1]
  interDF$var <- controlList[[n]]$var[1]
  interDF$ref_count <- interDF[,which(colnames(interDF)==interDF$ref[1])]
  interDF$var_count <- interDF[,which(colnames(interDF)==interDF$var[1])]
  
  caseList[[n]] <- interDF
}
```

# Prior filtering

Before doing any analyses, it might be beneficial (in terms of runtime) to filter out low-quality loci. MAGE's `prior_filter()` offers this functionality, leaving out loci based on a required minimal median coverage across all samples, a minimal number of samples, or a crudely estimated minimal minor allele frequency (just the % abundance of the variant allele over all RNAseq counts). After all, if the intent of a researcher is to study ASE effect which only occur in heterozygotes, then loci with an extremely low minor allele frequency will have very little of those to base analyses on. Also, if its \code{checkref_filter} argument is set to TRUE, it checks whether the total nucleotide counts truly suggest there's only two occuring alleles or whether there's heuristic evidence to the contrary, in which case the locus is removed (details are found on this function's help page).

For now, we'll restrict ourselves to just a minimal number of samples filter of 20; the first analysis is genotyping after all, and researchers might be interested in getting the most likely genotype for every sample regardless of median coverage or minor allele frequency across the entire locus (as MAGE is a population-level modeller a minimal number of samples of at least 20 is always advised). We apply no actual filtering on the case dataset but rather retain its data if it's retained in the control set as well, as comparison against controls is its main purpose. We do run \code{prior_filter} with no actual filter setting applied though, which reduces the function to removing samples having a zero-count for both reference- and variant-alleles (not removing these would give NA-results during several calculations in the pipeline). This reduces the toy datasets from 269 to 264 loci:

```{r}
for(n in names(controlList)){
  controlList[[n]] <- MAGE::prior_filter(controlList[[n]], min_median_cov = 0, 
    min_nr_samples = 20, checkref_filter = TRUE, prior_allelefreq_filter = FALSE, 
    min_PriorAlleleFreq = 0)
  caseList[[n]] <- MAGE::prior_filter(caseList[[n]], min_median_cov = 0, 
    min_nr_samples = 0, checkref_filter = FALSE, prior_allelefreq_filter = FALSE, 
    min_PriorAlleleFreq = 0)
  if(is.null(controlList[[n]])){
    caseList[[n]] <- NULL
  }
}
```

# Metaparameter estimation

Before getting to model fitting, we need to determine the population-level metaparameters shared across all loci, those being the inbreeding coefficient of the population and the sequencing error rate. These values can be set based on prior knowledge about the population and the sequencing protocol of choice, but MAGE's \code{AllelicMeta_est()} can estimate them based on observed *control* data, which is recommended in all cases for a good model fit; This function fits a simple binomial mixture model assuming no allelic bias (i.e. perfectly balanced expression of the reference- and variant allele in heterozygotes) to our read count data via expectation maximization:

$$
\small
\begin{aligned} 
\mathrm{PMF}(counts_{ref}|counts_{total}) = &\ P_{rr} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=1-SE) \ + \\
&\ P_{rv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=0.5) \ + \\
&\ P_{vv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=SE) \\
\end{aligned}
$$

With $(P_{rr}, P_{rv}, P_{vv})$ to-be-fitted genotype frequencies from which the inbreeding coefficient can be estimated, and SE a to-be-fitted sequencing error rate estimate. The function also returns a number of outputs that can be used as reliability measures, including an updated minor allele frequency estimate, median coverage and number of samples. The SE-estimate itself shouldn't be absurdly high either. Default cutoff values for these metrics are suggested below. We *don't* filter these loci out however; a suboptimal fit by \code{AllelicMeta_est()} may be remedied by the later beta-binomial fit allowing unequal heterozygous allele expression for actual genotyping and allelic bias detection.

```{r, results='asis'}
SE_vec <- c()   # Vector for saving reliable sequencing error rate estimates
F_vec <- c()    # Vector for saving reliable inbreeding coefficient estimates

pA_filt <- 0.15 # Loci with a low minor allele frequency are less reliable
SE_filt <- 0.035 # Loci which return a high estimated sequencing error are suspicious
NumSamp_filt <- 20 # Loci with a lot of samples are reliable for estimating inbreeding
MedianCov_filt <- 4 # Loci with low coverage are unreliable

for(n in names(controlList)){
  MetaEst_res <- MAGE::AllelicMeta_est(ref_counts = controlList[[n]]$ref_count, 
                  var_counts = controlList[[n]]$var_count)
  # You can store each locus' Sequencing Error (SE) and inbreeding coefficient (F) estimate 
  # in its dataframe, if you want:
  controlList[[n]]$est_SE <- MetaEst_res$SE
  controlList[[n]]$est_inbr <- MetaEst_res$F_inbr
  controlList[[n]]$allelefreq_0 <- MetaEst_res$allelefreq
  # Only take a locus' estimates into account if the locus is high-quality:
  if (!(MetaEst_res$allelefreq <= pA_filt || MetaEst_res$allelefreq >= (1 - pA_filt) 
        # allelefreq returns allele frequency of the ref-allele,
        # so just in case this is the minor allele on population level 
        # (even though it is the most expresse one across all RNAseq data),
        # we perform the filtering like this
      || MetaEst_res$SE > SE_filt) & nrow(controlList[[n]])>=NumSamp_filt & 
      median(controlList[[n]]$ref_count + controlList[[n]]$var_count) >= MedianCov_filt) {
      SE_vec <- c(SE_vec, MetaEst_res$SE)
      F_vec <- c(F_vec, MetaEst_res$F_inbr)
  }
}

SEmedian <- median(SE_vec)
Fmedian <- median(F_vec)

print(c(SEmedian, Fmedian))
```

These metaparameter estimates seem reasonable. Note that the sequencing error estimate may be quite a bit higher than expected from modern sequencing machines, but this is a good thing as an extremely low sequencing error metaparameter would make fitting of homozygous peaks later down the pipeline way too strict, failing to accomodate for even a single faulty read (Setting SE lower than 0.002 is not recommended). An inbreeding coefficient close to zero is a sign of a normal population and commonplace when working on human cancer data, but it could be more extreme in specific experimental setups.

# Genotyping RNAseq data & allelic bias detection

For both genotyping and cis-eqtl detection, we can use MAGE's `EMfit_betabinom_robust` function, which fits a beta-binomial mixture model using expectation maximization while allowing for unequal allelic expression in heterozygotes:

$$
\small
\begin{aligned} 
\mathrm{PMF}(counts_{ref}|counts_{total}) = &\ P_{rr} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=1-SE, \theta=\theta_{hom}) \ + \\
&\ P_{rv} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=\pi_{het}, \theta=\theta_{het}) \ + \\
&\ P_{vv} * {\tt pBetaBinom}(x=counts_{ref} \ |\ n=counts_{total}, \pi=SE, \theta=\theta_{hom}) \\
\end{aligned}
$$

With $(P_{rr}, P_{rv}, P_{vv})$ fitted genotype frequencies, $(\theta_{hom}, \theta_{het})$ fitted overdispersion parameters in homozygotes and heterozygotes respectively, and $\pi_{het}$ the heterozygous $\pi$ parameter indicating the expected reference allele fraction, thus allowing for unequal allelic expression in heterozygotes. \code{pBetaBinom} is a function included in the MAGE package for the beta-binomial PMF, using a $\pi, \theta$ parameterisation (probability of success i.e. reference read, and overdispersion, respectively) instead of the classical $\alpha, \beta$ parameterisation as given on e.g. https://en.wikipedia.org/wiki/Beta-binomial_distribution. The relationship between them is $\pi = \alpha/(\alpha+\beta)$ and  $\theta = 1/(\alpha+\beta)$, resulting in a $\pi$ between 0 and 1 and a $\theta$ between 0 and infinity.

We term unequal heterozygous allele expression capture by $\pi_{het}$ *Allelic Bias (AB)*. AB can be caused by both biological (e.g. cis-eqtl loci) and technical (e.g. alignment bias) mechanisms. The fit is robust in the sense that samples with a disproportionately high impact on the heterozygous distributional parameters (determined using parameter-MLEs leaving out that particular sample) are not used for the final fit, though they are retained in the output data and genotyped using the final model (while being marked as outlier). The reason for focussing on heterozygous distributional parameters is that these are still being fitted (the homozygous $\pi$ parameters are already fixed by the metaparameter SE) and are actually affected by allele-specific expression effects ($\pi_{het}$ can indicate AB and $\theta{het}$ can indicate allelic divergence, see later; meanwhile $\theta_hom$ is a nuisance parameter at best). Do note, however, that the robust fit is significantly slower than the non-robust one provided by the \code{EMfit_betabinom} function (a factor times the number of samples), and its merit for genotyping and AB-detection is limited, so one might opt for the latter instead. A robust fit is, however, very valuable for statistical inference on the overdispersion parameter $\theta$ (see the next section).

MAGE's functions have many optional input parameters to customize the fitting procedure, but throughout this tutorial and for most applications the defaults should suffice. Feel free to read up on all options on each function's help page. After performing the fit and writing results of interest into dataframes, we also perform a chi-square test assessing Hardy-Weinberg Equilibrium using our final genotypes and the previously determined inbreeding coefficient hyperparameter, to be used for filtering later. Finally, the code below has been adapted to running MAGE in parallel across multiple cores on a server. For this, just change the value or \code{NC} to the desired number of cores in the code below (for now it's at 1 as it will otherwise not work on a local Windows-installation). Parallellisation using the package \code{parallel} requires input data as a list with length equal to \code{NC} and all commands to be combined in a function, but if parallellization is not required the code below can be written much simpler using a for-loop.

```{r}
NC <- 1 # Number of Cores
NS <- length(controlList) 
spl <- c(0, cumsum(rep(floor(NS/NC),NC)+c(rep(1,NS-floor(NS/NC)*NC),
         rep(0,NC-NS+floor(NS/NC)*NC)))) # Helps in splitting input data
ParCTRL <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParCTRL[[i]] <- controlList[(spl[i]+1):(spl[i+1])]
}

BetaBinomGenotyping <- function(data){
  positions <- names(data)
  results <- data.frame()
  for (z in positions) {
    MAGEres <- MAGE::EMfit_betabinom_robust(data_counts = data[[z]], 
                                            SE = SEmedian, inbr = Fmedian)
    data[[z]] <- MAGEres$data_hash
    # median_AB also calculates a robust median AB,
    # besides the AB as determined during EMfit_betabinom_robust's fitting procedure.
    # This can be used as additional filter when detecting significant AB:
    med_AB <- MAGE::median_AB(data[[z]]$ref_count, data[[z]]$var_count, 
                               data[[z]]$allelefreq[1], Fmedian) 
    res_loc <- data.frame("position" = z, "probshift" = as.numeric(MAGEres$AB), 
      "LRT" = as.numeric(MAGEres$AB_lrt), "p" = as.numeric(MAGEres$AB_p),
      "quality" = MAGEres$quality, "allele.frequency" = data[[z]]$allelefreq[1], 
      "reference" = data[[z]]$ref[1], "variant" = data[[z]]$var[1], 
      "est_SE" = data[[z]]$est_SE[1], "coverage" = data[[z]]$coverage[1],
      "nr_samples" = nrow(data[[z]]), "median_AB" = med_AB, 
      "rho_rr" = MAGEres$rho_rr, "rho_rv" = MAGEres$rho_rv, "rho_vv" = MAGEres$rho_vv, 
      "theta_hom" = MAGEres$theta_hom, "theta_het" = MAGEres$theta_het,
      "theta_hom_NoShift" = MAGEres$theta_hom_NoShift, 
      "theta_het_NoShift" = MAGEres$theta_het_NoShift, stringsAsFactors = FALSE)
    results <- rbind(results, res_loc) # results; one position per line
  }
  results <- MAGE::HWE_chisquared(data = data, Fmedian, results = results)
  results$Chi2PVAL[is.na(results$Chi2PVAL)] <- 
  results$Chi2STAT[is.na(results$Chi2STAT)] <- -1
  return(list(data, results))
}

cl <- parallel::makeCluster(getOption("cl.cores", NC))
parallel::clusterExport(cl, c("Fmedian", "SEmedian"))
GenoFinData <- parallel::parLapply(cl, X = ParCTRL, fun = BetaBinomGenotyping)
parallel::stopCluster(cl)

ParCTRL <- lapply(GenoFinData, `[[`, 1)
controlList <- do.call(c, lapply(GenoFinData, `[[`, 1))
Geno_AB_res <- do.call("rbind", lapply(GenoFinData, `[[`, 2))
```

Calling upon MAGE's \code{pBetaBinom} function can results in a memory limit warning which can usually be ignored; most of the time it signifies that a certain locus' beta-binomial fit was so close to a regular binomial fit that the overdispersion parameter is extremely close to zero, leading to precision errors in beta-binomial calculations which will trigger MAGE to use regular binomial functions instead.

The genotyping results can now be retrieved from the `controlList` object, being included as the "genotypeN" column in each locus' dataframe:

```{r}
knitr::kable(head(controlList[["loc27"]][,c("locus_id", "sample_id", "ref", 
                                            "var", "genotypeN")]))
```

The `Geno_AB_res` dataframe contains the results of AB-detection, i.e. testing whether the reference allele expression ratio differs from a "perfectly balanced" ratio of 0.5 (i.e. is there a preference to express one of both alleles?). The "probshift" column contains the actual fitted allelic ratio while the "p" column contains the p-value of a likelihood-ratio-test assessing significant deviance from 0.5; both can be filtered on to retain only significantly allelically biased loci with a big enough effect size. 

Other columns contain additional filter criteria, like

* "median_AB": a median (robust) reference allele ratio estimate in case you want to play it extra safe in retaining significantly biased loci
* "quality": will contain "!" if a locus contained no heterozygous samples to detect AB on
* "allele.frequency": referring to the reference allele frequency across genotypes; you may not want to trust loci with extreme allele frequencies as these probably contain very little heterozygous individuals
* "coverage": contains the median coverage of all samples covering the locus)
* "nr_samples": number of samples covering the locus

The code below first retains loci showcasing statistically significant AB (controlling the false discovery rate at 0.05) which are of good quality. Then, we also construct a more high-fidelity set of loci, retaining only those of good quality with a fitted allelic ratio greater than 0.6 or smaller than 0.4, yet not more extreme than 0.9 or 0.1 because those could be the product of poorly fitted loci. The median ratio should also be larger than 0.6 or smaller than 0.4, allele frequency should not be more extreme then 0.9 or 0.1, median coverage should be over 10 and number of samples should be over 80. These are reasonable filter criteria, though their specific values are dependent on each end user's desires en the specific dataset used (e.g. requiring over 80 samples is perhaps too stringent for smaller datasets and too liberal for larger ones).

In BOTH CASES, we *only retain loci that are in Hardy-Weinberg Equilibrium (HWE)* according to the chi square test performed earlier. Since we retain only those loci for which we can't find sufficient evidence against the null hypothesis of HWE, this is a statistically weak conclusion, yet in scientific research a cut-off of the p-value of this test of 0.001 is often used to filter out loci with which something seems wrong (see e.g. \insertCite{sha}{MAGE}, \insertCite{teo}{MAGE}, \insertCite{rohlfs}{MAGE}); such extreme deviations from HWE should not occur in a normal population.

```{r, results='asis'}
# Statistical evidence for significant AB at the 5% FDR level:
print(paste(Geno_AB_res$position[p.adjust(Geno_AB_res$p, method = "BH") < 0.05 & 
      Geno_AB_res$quality != "!" & Geno_AB_res$Chi2PVAL >= 0.001], collapse = ", "))

```

```{r, results='asis'}
# Statistical evidence for significant AB at the 5% FDR level,
# only retaining reliable (high-quality) loci with a large enough effect size:
print(paste(Geno_AB_res$position[p.adjust(Geno_AB_res$p, method = "BH") < 0.05 & 
  Geno_AB_res$quality != "!" & (Geno_AB_res$probshift > 0.6 | Geno_AB_res$probshift < 0.4) 
  & Geno_AB_res$probshift < 0.9 &  Geno_AB_res$probshift > 0.1 & (Geno_AB_res$median_AB > 0.6 | 
  Geno_AB_res$median_AB < 0.4) & Geno_AB_res$allele.frequency < 0.9 & 
  Geno_AB_res$allele.frequency > 0.2 & Geno_AB_res$coverage > 10 & 
  Geno_AB_res$nr_samples > 80 & Geno_AB_res$Chi2PVAL >= 0.001], collapse = ", "))

```

MAGE contains a function to plot histograms of the reference allele fraction across samples for a specific locus, while also including PMF plots of both the unshifted (assuming no AB) and shifted models. The theta-parameters for the unshifted model are included in the previous \code{EMfit_betabinom_robust()} output as they are calculated anyway to test for a significant shift using a likelihood ratio test. This plot can illustrate just how much better the shifted model fits to the data, as illustrated below. 

Keep in mind, however, that for a beta-binomial PMF, its shape (variance) depends on the coverage it is calculated on even when normalized from the reference allele count to a 0-to-1 reference allele fraction, such as on the x-axis in the plot below. As such, we have to pick a coverage on which to base the PMF plots (\code{ScaleCount} argument). The median coverage across samples is a good pick for this, but the PMF plots may not fit certain data points of the histogram bars all that well if these correspond to samples with outlying coverage, even if the actual fitted beta-binomial model fits them extremely well. This is because the fitted model allows for a flexible per-sample coverage parameter; for our PMF plot we have to pick just one.

To accomodate for this, one might opt to set the \code{ScaleHist} argument to TRUE, which will re-calculate observed per-sample reference allele fraction that are to be plotted in the histogram using the MAGE's beta-binomial quantile function given the observed per-sample data probability as input, but while using \code{ScaleCount} total counts (i.e. we transform observed reference allele fractions to the fractions we "would have observed" - i.e. with the same cumulative distribution function value - given \code{ScaleCount} observed reads, assuming the fitted beta-binomial model is correct). It's up to the end user to use this argument, though for a comfortable interpretation of the plots it's recommended if the per-sample coverage is very variable.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
PlotData <- controlList[["loc11"]]
PlotData_eqtl <- Geno_AB_res[Geno_AB_res$position=="loc11",]

loc_plot <- MAGE::MAGE_EMfitplot(ref_counts=PlotData$ref_count, 
  var_counts=PlotData$var_count, pr=PlotData_eqtl$rho_rr, prv=PlotData_eqtl$rho_rv,
  pv=PlotData_eqtl$rho_vv, theta_hom=PlotData_eqtl$theta_hom, 
  theta_het=PlotData_eqtl$theta_het, pr_NoShift=mean(PlotData$prr_H0), 
  prv_NoShift=mean(PlotData$prv_H0), pv_NoShift=mean(PlotData$pvv_H0), 
  theta_hom_NoShift=PlotData_eqtl$theta_hom_NoShift, 
  theta_het_NoShift=PlotData_eqtl$theta_het_NoShift, 
  probshift=as.numeric(PlotData_eqtl$probshift), SE=SEmedian, 
  ScaleCount = PlotData_eqtl$coverage, ScaleHist = TRUE, plot_NoShift = TRUE, nbins = 30)

loc_plot
```

The plotting function contains some customization options, the plot above mainly using default colors with the PMF of the shifted fit split up according to genotype. One can e.g. choose to plot one combined unshifted PMF, and to not plot the unshifted fit, via the code below (once again using default colors, though these can all be changed):

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
loc_plot <- MAGE::MAGE_EMfitplot(ref_counts=PlotData$ref_count, 
  var_counts=PlotData$var_count, pr=PlotData_eqtl$rho_rr, prv=PlotData_eqtl$rho_rv,
  pv=PlotData_eqtl$rho_vv, theta_hom=PlotData_eqtl$theta_hom, 
  theta_het=PlotData_eqtl$theta_het, pr_NoShift=mean(PlotData$prr_H0), 
  prv_NoShift=mean(PlotData$prv_H0), pv_NoShift=mean(PlotData$pvv_H0), 
  theta_hom_NoShift=PlotData_eqtl$theta_hom_NoShift,
  theta_het_NoShift=PlotData_eqtl$theta_het_NoShift, 
  probshift=as.numeric(PlotData_eqtl$probshift), SE=SEmedian, 
  ScaleCount = PlotData_eqtl$coverage, ScaleHist = TRUE, plot_NoShift = FALSE,
  SplitPeaks = FALSE, nbins = 30)

loc_plot
```

# Differential allelic divergence detection

In the context of MAGE, we see tumor-specific events such Copy Number Alterations, aberrant hypermethylation, promotor mutation and silencing events of tumor suppressor genes as mechanisms that will affect one allele, or both alleles in a different way, per sample. However, these mechanisms show no preference towards either allele on a population-level, acting mainly as an added source of variance on the observed reference allele fraction captured by $\theta_{het}$ while leaving $\pi_{het}$ unchanged. Calling this variance *Allelic Divergence (AD)*, we term its change (increase) in disease due to, amongst others, the mechanisms listed above *differential Allelic Divergence (dAD)*.

Detecting dAD between to populations can be done via likelihood ratio test, comparing: (1) a model fitting ALL data, both control and case, as one beta-binomial mixture model, with (2) a model allowing the control- and tumor data to have their separate $\theta$ parameters for the heterozygous samples, yet keeping the $\pi$ parameter constant. These two models' parameters are distinguished by respectively `H0` (null hypothesis) and `H1` (alternative hypothesis) in the code below. The pivotal new function is \code{EMfit_betabinom_popcomb} which fits the model assuming different heterozygote-thetas while keeping all other parameters shared (i.e. the `H1` model):

```{r warning=FALSE}
ParCASE <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParCASE[[i]] <- caseList[(spl[i]+1):(spl[i+1])]
}
ParTOT <- vector(mode = "list", length=NC)
for(i in 1:NC){ # Put the splitted input data into a list for parallellisation
  ParTOT[[i]] <- list(ParCTRL[[i]], ParCASE[[i]])
}

dAD_analysis <- function(data){
  
  controlListP <- data[[1]]
  caseListP <- data[[2]]
  positions <- names(controlListP)
  
  dAD_res <- data.frame(LocName = names(controlListP), PiFitH0 = 0, PiFitH1 = 0, ThetaH0 = 0,
    ThetaCTRL_H1 = 0, ThetaCASE_H1 = 0, RhoH0 = 0, RhoCTRL = 0, RhoCASE = 0, LRTpval = 0, 
    NumHetCTRL = 0, NumHetCASE = 0, RobFlagCTRL = "", RobFlagCASE = "", HWECTRL = 0,
    HWECASE = 0, CovCTRL_mean = 0, CovCASE_mean = 0, CovCTRL_med = 0, CovCASE_med = 0,
    NumOutCTRL = 0, NumOutCASE = 0, QualityCTRL = "N", QualityCASE = "N", pr = 0, 
    prv = 0, pv = 0, ThetaHom = 0)

  for (LOC in positions) {
    
    CTRL_DF <- data.frame("ref_count" = controlListP[[LOC]]$ref_count, 
                       "var_count" = controlListP[[LOC]]$var_count, "isCase" = 0)
    CASE_DF <- data.frame("ref_count" = caseListP[[LOC]]$ref_count, 
                       "var_count" = caseListP[[LOC]]$var_count, "isCase" = 1)
    
    # Previously, we used EMfit_betabinom_robust() to ROBUSTLY fit our models by removing
    # outliers via Cook's distance. Now however, we're working with data from two different
    # sources (control- and case tissue) and have no way of knowing in advance how similar
    # these are. As such, outlier detections should happen ON BOTH SETS SEPARATELY yet the
    # two hypotheses we'll be fitting share all or some parameters between the two. 
    # For this, we can use the EMfit_betabinom_robust() function with its "fitH0" set to 
    # FALSE, which will not complete the entire eqtl-detection pipeline, but will cut it 
    # short before fitting the unshifted model. By running this function on both our 
    # datasets separately in advance of the dAD-relevant fits, we ensure correct outlier
    # detection Ã¡nd the use of the same dataset in our upcoming likelihood ratio tests.
  
    # 1. Detect and extract outliers using EMfit_betabinom_robust()
    OUTfitCTRL <- MAGE::EMfit_betabinom_robust(data_counts = CTRL_DF, SE = SEmedian, 
                                               inbr = Fmedian, fitH0 = FALSE)
    OUTfitCASE <- MAGE::EMfit_betabinom_robust(data_counts = CASE_DF, SE = SEmedian,
                                               inbr = Fmedian, fitH0 = FALSE)
    OUTfitCTRL_DH <- OUTfitCTRL$data_hash; OUTfitCASE_DH <- OUTfitCASE$data_hash
    CTRL_DF$Outlier <- OUTfitCTRL_DH$Outlier; CASE_DF$Outlier <- OUTfitCASE_DH$Outlier
    CurDF <- rbind(CTRL_DF, CASE_DF)
    # These results are also handy for an estimation of the number of heterozygotes in 
    # controls and tumors AND the number of outliers, both good filter criteria.
    # We can also include a "RobFlag" which gives more information about outlier detection 
    # (e.g. none detected, or so unreasonably many that none were removed)
    dAD_res$NumHetCTRL[dAD_res$LocName == LOC] <- sum(OUTfitCTRL_DH$prv)
    dAD_res$NumHetCASE[dAD_res$LocName == LOC] <- sum(OUTfitCASE_DH$prv)
    dAD_res$NumOutCTRL[dAD_res$LocName == LOC] <- sum(CTRL_DF$Outlier)
    dAD_res$NumOutCASE[dAD_res$LocName == LOC] <- sum(CASE_DF$Outlier)
    dAD_res$RobFlagCTRL[dAD_res$LocName == LOC] <- OUTfitCTRL$RobFlag
    dAD_res$RobFlagCASE[dAD_res$LocName == LOC] <- OUTfitCASE$RobFlag
  
  
    # 2. Perform one fit on the entire (non-outlying) data, i.e. all parameters shared,
    # i.e. assuming no dAD; the null hypothesis in dAD detection.
    # Remark this uses the non-robust fitting function, since outliers were already detected.
    NOdAD_fit <- MAGE::EMfit_betabinom(data_counts = CurDF[CurDF$Outlier == 0,], 
      SE = SEmedian, inbr = Fmedian, fitH0 = FALSE)
    NOdAD_fit_DF <- NOdAD_fit$data_hash
    PiH0 <- NOdAD_fit$AB
    rho_rr <- NOdAD_fit$rho_rr; rho_rv <- NOdAD_fit$rho_rv; rho_vv <- NOdAD_fit$rho_vv
    ThetaHomH0 <- NOdAD_fit$theta_hom; ThetaHetH0 <- NOdAD_fit$theta_het
  
    dAD_res$PiFitH0[dAD_res$LocName == LOC] <- PiH0
    dAD_res$ThetaH0[dAD_res$LocName == LOC] <- ThetaHetH0
  
  
    # 3. Perform a fit on the (non-outlying) data allowing separate theta_het parameters 
    # for control- and case-data
    FullFit <- MAGE::EMfit_betabinom_popcomb(data_counts = CurDF[CurDF$Outlier == 0,],
      SE = SEmedian, inbr = Fmedian, probshift_init = PiH0)
    ParamVec <- FullFit$ParamVec
    dAD_res$PiFitH1[dAD_res$LocName == LOC] <- ParamVec["probshift"]
    dAD_res$ThetaCTRL_H1[dAD_res$LocName == LOC] <- ParamVec["theta_het_control"]
    dAD_res$ThetaCASE_H1[dAD_res$LocName == LOC] <- ParamVec["theta_het_case"]
    
  
    # 4. Perform the Likelihood Ratio Test for dAD detection
    # Likelihood of fit with all parameters shared:
    LikTot <- MAGE::pmf_betabinomMix(CurDF[CurDF$Outlier==0,]$ref_count, 
      CurDF[CurDF$Outlier==0,]$var_count, probshift = PiH0, SEmedian, rho_rr, rho_vv, rho_rv, 
      theta_hom = ThetaHomH0, theta_het = ThetaHetH0)
    # Likelihood of fit with separate theta_het (calculated in two steps because of the 
    # different theta)
    LikCTRL <- MAGE::pmf_betabinomMix(CTRL_DF[CTRL_DF$Outlier==0,]$ref_count, 
      CTRL_DF[CTRL_DF$Outlier==0,]$var_count, probshift = ParamVec["probshift"], SEmedian,
      ParamVec["pr"], ParamVec["pv"], ParamVec["prv"], theta_hom = ParamVec["theta_hom"],
      theta_het = ParamVec["theta_het_control"])
    LikCASE <- MAGE::pmf_betabinomMix(CASE_DF[CASE_DF$Outlier==0,]$ref_count, 
      CASE_DF[CASE_DF$Outlier==0,]$var_count, probshift = ParamVec["probshift"], SEmedian,
      ParamVec["pr"], ParamVec["pv"], ParamVec["prv"], theta_hom = ParamVec["theta_hom"], 
      theta_het = ParamVec["theta_het_case"])
    lrtstat <- -2 * (sum(log(LikTot)) - sum(log(c(LikCTRL, LikCASE))))
    LRTpval <- pchisq(lrtstat, df = 1, lower.tail = F)
    dAD_res$LRTpval[dAD_res$LocName == LOC] <- LRTpval
  
  
    # 5. Fill out the results dataframe
    dAD_res$QualityCTRL[dAD_res$LocName == LOC] <- # spot bad quality data
      OUTfitCTRL$quality; dAD_res$QualityCASE[dAD_res$LocName == LOC] <-OUTfitCASE$quality 
    # Test HWE on both the control and tumor data:
    HWEtest_CTRL <- MAGE::HWE_chisquared(Fmedian = Fmedian, data = OUTfitCTRL_DH)
    dAD_res$HWECTRL[dAD_res$LocName == LOC] <- HWEtest_CTRL$PVAL
    HWEtest_CASE <- MAGE::HWE_chisquared(Fmedian = Fmedian, data = OUTfitCASE_DH)
    dAD_res$HWECASE[dAD_res$LocName == LOC] <- HWEtest_CASE$PVAL
    # Mean and median coverages:
    dAD_res$CovCTRL_mean[dAD_res$LocName == LOC] <- 
      mean(CTRL_DF[CTRL_DF$Outlier==0,]$ref_count + CTRL_DF[CTRL_DF$Outlier==0,]$var_count)
    dAD_res$CovCASE_mean[dAD_res$LocName == LOC] <- 
      mean(CASE_DF[CASE_DF$Outlier==0,]$ref_count + CASE_DF[CASE_DF$Outlier==0,]$var_count)
    dAD_res$CovCTRL_med[dAD_res$LocName == LOC] <- 
      median(CTRL_DF[CTRL_DF$Outlier==0,]$ref_count + CTRL_DF[CTRL_DF$Outlier==0,]$var_count)
    dAD_res$CovCASE_med[dAD_res$LocName == LOC] <- 
      median(CASE_DF[CASE_DF$Outlier==0,]$ref_count + CASE_DF[CASE_DF$Outlier==0,]$var_count)
    dAD_res$pr[dAD_res$LocName == LOC] <- ParamVec["pr"]
    dAD_res$prv[dAD_res$LocName == LOC] <- ParamVec["prv"]
    dAD_res$pv[dAD_res$LocName == LOC] <- ParamVec["pv"]
  
    dAD_res$ThetaHom[dAD_res$LocName == LOC] <- ParamVec["theta_hom"]
  }

  dAD_res$HWECTRL[is.na(dAD_res$HWECTRL)] <- 
  dAD_res$HWECTRL[is.na(dAD_res$HWECTRL)] <- -1
  
  dAD_res$RhoH0 <- 1/((1/dAD_res$ThetaH0)+1)
  dAD_res$RhoCTRL <- 1/((1/dAD_res$ThetaCTRL_H1)+1)
  dAD_res$RhoCASE <- 1/((1/dAD_res$ThetaCASE_H1)+1)
    
  return(dAD_res)
}

cl <- parallel::makeCluster(getOption("cl.cores", NC))
parallel::clusterExport(cl, c("Fmedian", "SEmedian"))
dADFinData <- parallel::parLapply(cl, X = ParTOT, fun = dAD_analysis)
parallel::stopCluster(cl)

dAD_res <- do.call("rbind", dADFinData)
  
```

In `dAD_res`, both \code{theta}- and \code{rho}-values are listed, which both represent the overdispersion parameter; $\theta$ just has a range from 0 to infinity while $\rho$ has been rescaled to go from 0 to 1, which is more convenient for some analyses, such as plotting. When studying differential allelic divergence, one usually tries to look for "disturbed regions" by plotting the results in `dAD_res` among the length of chromosomes (e.g. plotting tumor rho-values along the chromosomes while also marking the actually statistically relevant loci in some way), as will be demonstrated at the end of this section.

That being said, you can also just extract loci showing differential allelic divergence from `dAD_res` using the `LRTpval` column to retain statistically significant results. Defining an effect size is a bit trickier; you'd expect to be able to do it on the overdispersion-parameter-values of cases compared to controls, but how would one go about it? When both are very small (e.g. rho ~ 10^-7 range), one being 10 times as large as the other doesn't mean that much, but when both are quite large (e.g. rho ~ 10^-2 range) this does make a very large difference. Subtracting one from the other has an equally unclear interpretation regarding effect size. The effect of the overdispersion parameter on the actual variance also depends on the pi parameter, which further complicates things.

These kind of difficulties makes us prefer a "looser" approach such as detecting affected regions on plots rather than strict statistical procedures. But if you wanted to e.g. only retain loci that show a strong deviation from a regular binomial regarding its variance, using a rho-cutoff of, say, 0.1, can do. Such cutoffs are based on experience and can be played around with. In the code below, we retain such good-quality loci which are also statistically significant and have a median coverage of at least 10, at least 15 heterozygous samples, and samples conform Hardy-Weinberd Equilibrium in both control- and tumor-data. Also, note that for true induced allelic imbalance, overdispersion in the tumor samples needs to be larger than in the control samples.

```{r, results='asis'}
print(paste(dAD_res$LocName[dAD_res$RhoCASE >= 0.1 & p.adjust(dAD_res$LRTpval, 
  method = "BH") < 0.05 & dAD_res$CovCTRL_med >= 10 & dAD_res$CovCASE_med >= 10 & 
  dAD_res$NumHetCTRL >= 15 & dAD_res$NumHetCASE >= 15 & dAD_res$HWECTRL >= 0.001 & 
  dAD_res$HWECASE >= 0.001 & dAD_res$RhoCASE > dAD_res$RhoCTRL], collapse = ", "))
```

The previously used \code{MAGE_EMfitplot} function can also be used to vizualize the dAD detection results via seperate plots of the control- and case-population utilizing their separately fitted overdispersion parameters. Here, it's important to set the input argument \code{ScaleCount} to the same value for both plots, so the width of the PMFs can actually be compared correctly.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
CTRL_DF <- controlList[["loc85"]]
CASE_DF <- caseList[["loc85"]]
PlotData_dAD <- dAD_res[dAD_res$LocName=="loc85",]

dAD_plot1 <- MAGE::MAGE_EMfitplot(ref_counts=CTRL_DF$ref_count, var_counts=CTRL_DF$var_count,
  pr=PlotData_dAD$pr, prv=PlotData_dAD$prv, pv=PlotData_dAD$pv, 
  theta_hom = PlotData_dAD$ThetaHom, theta_het = PlotData_dAD$ThetaCTRL_H1, 
  probshift = PlotData_dAD$PiFitH1, SE = SEmedian, ScaleCount = 50, ScaleHist = TRUE, 
  nbins = 30, plot_NoShift = FALSE)

dAD_plot2 <- MAGE::MAGE_EMfitplot(CASE_DF$ref_count, CASE_DF$var_count, pr=PlotData_dAD$pr,
  prv=PlotData_dAD$prv, pv=PlotData_dAD$pv, theta_hom = PlotData_dAD$ThetaHom, 
  theta_het = PlotData_dAD$ThetaCASE_H1, probshift = PlotData_dAD$PiFitH1, SE = SEmedian,
  ScaleCount = 50, ScaleHist = TRUE, nbins = 30, plot_NoShift = FALSE)

gridExtra::grid.arrange(dAD_plot1, dAD_plot2, ncol=2)
```

The plot above is of a locus showcasing very clear differential allelic divergence, with the reference allele fraction in heterozygous case samples being way more variable, probably due to case-specific effects.

As promised, to conclude, this section showcases a plot of dAD results across a chromosome using the \code{MAGE_ADChromplot} function. The toy data isn't very fit for this, however (no chromosomal location, too small), so included with the toy data is some thoroughly processed data to illustrate the plot on. The inputs needed are $\rho$ values in both controls and cases along with differential expression data (log2 fold changes) which can also be inferred from raw RNAseq counts. \code{MAGE_ADChromplot} optionally even accepts additional hypermethylation and copy-number-alteration data for even more extensive plots. As this is for illustrative purposes only, we just make the plot using the provided datasets below, but more details can be learned from \code{MAGE_ADChromplot}'s help page. The function returns several plot components (including legends) as separate objects due to its complexity, so it's up to the end user to arrange them into a suitable composition.

As a final remark, many of the measures in this plot (differential expression, hypermethylation, copy number alterations) are usually obtained at the gene-level rather than the SNP/locus level. To make the results of different analyses more compareable, MAGE's per-locus p-values can be combined using the \code{combine_p_gene} function. For this, we recommend using the geometric mean as a nice balance between more conservative (arithmetic mean, maximum) and liberal (harmonic mean, minimum) methods for combining p-values, and the different loci should be weighted as well, for which we recommend $\small \sqrt{median \ locus \ coverage \ * \ estimated \ number \ of \ heterozygous \ samples }$, the latter being an output of MAGE's beta-binomial mixture model fits via the heterozygote genotype frequency times the number of samples. For p-values concerning a comparison between a control- and case-population, one can e.g. use the minimum of this weight across populations.

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
ChromPlot <- MAGE::MAGE_ADChromplot(AD_Data, DE_Data, Meth_Data, CNAgain_Data, CNAloss_Data,
                                    pvalSIG = 0.05, roll_median = 15)

ChromPlot[["ADDE_plot"]] / ChromPlot[["MethCNA_plot"]] / (ChromPlot[["LEG1"]] + 
  ChromPlot[["LEG2"]] + ChromPlot[["LEG3"]]) + 
  patchwork::plot_layout(heights = c(2,1,0.5), widths = c(1,1,1))
```

The histograms in the top panes show (rolling medians) of the overdisperstion ($\rho$) parameters in controls (black) and cases (colored). The case-$\rho$ is colored according to the (already 5% FDR corrected) p-value testing for significant dAD in cases versus controls, while the relative heigts of these histograms provide an illustration of the effect size. The remaining plot elements are explained in the legend.

# Detecting imprinted and differentially imprinted loci

Imprinting analyses in MAGE are quite different from the analyses up until now in that they don't rely on beta-binomial modelling and make different assumptions about the data. They are, generally speaking, a bit more robust because imprinting is usually an extreme phenomenon, i.e. heterozygous samples of imprinted loci will almost always "seem like" homozygous samples because one of the alleles has been silenced almost entirely. Partial imprinting can exist and MAGE can detect it, but this phenomenon is not modeled as thoroughly as the previous ones in this vignette.

First off we assume samples show no AB and are conform Hardy-Weinberg-Equilibrium while taking imprinting into account, which is tested by \code{symmetry_gof}. It does this by assessing whether the amount of samples with a reference allele fraction > 0.5 (and so also the amount with the fraction <= 0.5) is equal to that expected under HWE assuming no AB using a chi square test. This may seem kind of heuristic at first and prone to fail if there is even a little AB, but remember we want to mainly retain imprinted loci, meaning heterozygous samples should behave as one of both homozygotes (and so either clearly > 0.5 or <= 0.5 in terms of reference allele fraction) so this filter is very fitting for retaining HWE-conform samples that are at the same time likely to showcase imprinting. A good filter setting is requiring the p-value to be at least 0.05.

After making these assumptions, \code{imprinting_est} detects imprinted loci in the remaining control samples by first assuming an unshifted binomial mixture model, then splitting up the heterozygous samples in two separate groups according to a degree of imprinting $i$ and varying this $i$ from 0 to 1 in a for-loop, calculating likelihoods along the way to perform a likelihood-ratio test to check for differential imprinting (i.e. compare the most likely model with the non-imprinted one; $i=0$). For completeness, the imprinted PMF is:

$$
\small
\begin{aligned} 
\mathrm{PMF}(&counts_{ref}|counts_{total}) =  P_{rr} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=1-SE) \ + \\
& 0.5P_{rv} * {\tt pbinom} \left( x=counts_{ref} \ |\ n=counts_{total}, p=\frac{0.5-\frac{i}{2}}{1-\frac{i}{2}}(1-SE)+\frac{0.5}{1-\frac{i}{2}}SE\right) \ + \\
& 0.5P_{rv} * {\tt pbinom} \left( x=counts_{ref} \ |\ n=counts_{total}, p=\frac{0.5-\frac{i}{2}}{1-\frac{i}{2}}SE+\frac{0.5}{1-\frac{i}{2}}(1-SE)\right) \ + \\
& P_{vv} * {\tt pbinom}(x=counts_{ref} \ |\ n=counts_{total}, p=SE)
\end{aligned}
$$

After retaining only significantly and sufficiently imprinted loci (5% FDR level, degree of imprinting at least 0.6 and median degree of imprinting at least 0.8 for robustness; see \code{median_imprinting} for the latter's calculation) using the \code{final_filter} function, we use \code{LOItest_logreg()} to check whether their corresponding case samples show a significantly higher number of apparently heterozygous samples via a logistic regression test (more details can be found on the function's help page). If this is true, it indicates that a significant fraction of heterozygous samples in tumor tissue "lost their imprinting" and actually started expressing both alleles again. We call these loci "differentially imprinted". Comment in the code below provide additional insight into the imprinting analysis pipeline:

```{r warning=FALSE}
# Perform filtering using symmetry_gof()
# Notice we use allelefreq_0 as input in this function, which is the allele frequency as 
# estimated by an UNSHIFTED binomial mixture model using AllelicMeta_est() earlier in this 
# vignette, which is what symmetry_GOF assumes as well so it's only fitting.
# We also enforce that this allelefreq_0 can not be more extreme than 0.15 or 0.85, 
# because detecting imprinting would be very hard otherwise.
ImprData <- controlList
for(LOC in names(controlList)){
  if (ImprData[[LOC]]$allelefreq_0[1] <= 0.15 || 
      ImprData[[LOC]]$allelefreq_0[1] >= (1 - 0.15)) {
    ImprData[[LOC]] <- NULL
  } else {
    ImprData[[LOC]]$sym <- MAGE::symmetry_gof(ImprData[[LOC]]$ref_count, 
      ImprData[[LOC]]$var_count, ImprData[[LOC]]$allelefreq_0[1])
    if (ImprData[[LOC]]$sym[1] <= 0.05) {
      ImprData[[LOC]] <- NULL
    }
  }
}


# Detect imprinted control loci
impr_res <- data.frame()
for(LOC in names(ImprData)){
  i_results <- MAGE::imprinting_est(ImprData[[LOC]]$ref_count, ImprData[[LOC]]$var_count, 
                allelefreq = ImprData[[LOC]]$allelefreq_0[1], SE = SEmedian, inbr = Fmedian)
  # An additional robustified "median imprinting" across samples to be used as possible 
  # additional filter criterion:
  med_imp <- MAGE::median_imprinting(ImprData[[LOC]]$ref_count, ImprData[[LOC]]$var_count, 
              allelefreq = ImprData[[LOC]]$allelefreq_0[1], inbr = Fmedian)
  results_z <- data.frame("position" = ImprData[[LOC]]$locus_id[1], "LRT" = i_results$LRT, 
    "p" = i_results$p_value, "estimated.i" = i_results$est_i, "allele.frequency" = 
    ImprData[[LOC]]$allelefreq_0[1], "reference" = ImprData[[LOC]]$ref[1], "variant" = 
    ImprData[[LOC]]$var[1], "med_cov" = ImprData[[LOC]]$coverage[1], "nr_samples" = 
    nrow(ImprData[[LOC]]), "GOF" = i_results$GOF_likelihood, "symmetry" = 
    ImprData[[LOC]]$sym[1], "med_impr" = med_imp, stringsAsFactors = FALSE)
  impr_res <- rbind(impr_res, results_z)
}
# Retain significantly imprinted loci (5% FDR) utilizing some additional filters, amongst
# which a custom  Goodness-Of-Fit which more or less corresponds to a locus' likelihood of
# the imprinted model*coverage; 0.8 is a good cutoff. Other filter criteria are imprinting 
# (0.6) and median imprinting (0.8)
impr_res_FIN <- MAGE::final_filter(data_hash=NULL, impr_res, results_wd=NULL, gof_filt = 0.8, 
  med_impr_filt = 0.8, i_filt = 0.6, file_all = FALSE, file_impr = FALSE, 
  file_all_counts = FALSE, file_impr_counts = FALSE)


# From among actually imprinted loci, detect differential expression in case data
pos_impr <- as.character(impr_res_FIN$position)
p_DI_df <- impr_res_FIN
p_DI_df$DI_pval <- 1
for(LOC in pos_impr){
  CData <- controlList[[LOC]]
  TData <- caseList[[LOC]]
  p_DI <- MAGE::LOItest_logreg(CData$ref_count, CData$var_count, 
                                  TData$ref_count, TData$var_count)$p.value
  p_DI_df$DI_pval[p_DI_df$position == LOC] <- p_DI
}

```

We can take a look at the loci that are significantly and sufficiently imprinted (i.e. the output of \code{final_filter} from above):

```{r, results='asis'}
print(paste(impr_res_FIN$position, collapse = ", "))
```

From among this set, a part is actually differentially imprinted in case samples, which is indicated by the p-value in the `DI_pval` column in the `p_DI_df` dataframe. We can control loss-of-imprinting detection, for a change, at the 5% FWER level (actually feasible here due to the small number of actually imprinted loci remaining, though we're looking at a small toy dataset here; this isn't necessarily the case in larger experiments):

```{r, results='asis'}
print(paste(p_DI_df$position[p.adjust(p_DI_df$DI_pval, method = "holm") < 0.05], 
            collapse = ", "))
```

We can plot histograms of both the control- and tumor allele fractions; locus 202 is obviously imprintend in controls and loses a lot of this imprinting in cases (controls are plotted using \code{MAGE_imprintplot} to visualize imprinting results, but since an imprinting-estimation never happens on the case data - we just detect a change in heterozygosity via logistic regression - we simple make a histogram for cases instead).

```{r, out.width='100%', fig.width=8.5, fig.height=5, fig.align='center', warning=FALSE, results = 'asis', message=FALSE}
HistCTRL <- MAGE::MAGE_imprintplot(controlList[["loc202"]]$ref_count, 
  controlList[["loc202"]]$var_count, 
  allelefreq = impr_res_FIN$allele.frequency[impr_res_FIN$position=="loc202"],
  impr = impr_res_FIN$estimated.i[impr_res_FIN$position=="loc202"],
  SE = SEmedian, inbr = Fmedian, plot_NoImpr = TRUE, SplitPeaks = FALSE) + 
  ggplot2::ggtitle("Control")

RatioCASE <- caseList[["loc202"]]$ref_count / 
  (caseList[["loc202"]]$ref_count + caseList[["loc202"]]$var_count)
CASEDat <- data.frame("Ratio" = RatioCASE)
HistCASE <- ggplot2::ggplot() + ggplot2::geom_histogram(data = CASEDat, ggplot2::aes(Ratio),
  bins = 50) + ggplot2::labs(x="Reference allele fraction", y="Frequency") + 
  ggplot2::ggtitle("Case")

gridExtra::grid.arrange(HistCTRL, HistCASE, ncol=2)

```

# Session info

```{r}
sessionInfo()
```
